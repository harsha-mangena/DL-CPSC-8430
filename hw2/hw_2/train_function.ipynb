{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e751e57-13a8-48e3-adc3-d2f6b501c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a14020-674d-4200-9f8c-b2d6dcecc559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78170bb3-9f86-4923-b8b4-b38cbda545f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.special import expit\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "562d8757-5c9c-416d-aac9-2252e7a50dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def __preprocess():\n",
    "    filepath = 'data/'\n",
    "    # Initialize a Counter object to simplify word counting\n",
    "    word_count = Counter()\n",
    "\n",
    "    with open(filepath + 'training_label.json', 'r') as f:\n",
    "        file = json.load(f)\n",
    "\n",
    "    # Process each sentence in the data\n",
    "    for d in file:\n",
    "        for s in d['caption']:\n",
    "            # Simplify punctuation removal and split in one step\n",
    "            word_sentence = re.sub('[.!,;?]', ' ', s).lower().split()\n",
    "            # Update the word counts for all words in the sentence\n",
    "            word_count.update(word_sentence)\n",
    "\n",
    "    # Filter out words with occurrences fewer than 5\n",
    "    word_dict = {word: count for word, count in word_count.items() if count > 4}\n",
    "\n",
    "    # Initial tokens for special purposes\n",
    "    useful_tokens = [('<PAD>', 0), ('<SOS>', 1), ('<EOS>', 2), ('<UNK>', 3)]\n",
    "    \n",
    "    # Create index-to-word and word-to-index dictionaries, incorporating special tokens\n",
    "    i2w = {i + len(useful_tokens): w for i, w in enumerate(word_dict)}\n",
    "    w2i = {w: i + len(useful_tokens) for i, w in enumerate(word_dict)}\n",
    "    for token, index in useful_tokens:\n",
    "        i2w[index] = token\n",
    "        w2i[token] = index\n",
    "\n",
    "    return i2w, w2i, word_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4158db7-9fb0-46a9-baf8-cf516b2c1e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_split(sentence, word_dict, w2i):\n",
    "    # Normalize and split the sentence into words\n",
    "    words = re.sub(r'[.!,;?]', ' ', sentence).split()\n",
    "\n",
    "    # Convert words to their corresponding indices in w2i, defaulting to <UNK> index if not found\n",
    "    indexed_sentence = [w2i.get(word, 3) for word in words]\n",
    "\n",
    "    # Prepend <SOS> and append <EOS> tokens\n",
    "    indexed_sentence = [1] + indexed_sentence + [2]\n",
    "\n",
    "    return indexed_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38d86b10-24d9-48d2-8a69-963b094dbf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(label_file, word_dict, w2i):\n",
    "    # Define the full path to the label file\n",
    "    label_json_path = f'data/{label_file}'\n",
    "    annotated_captions = []\n",
    "\n",
    "    # Open and load the label file\n",
    "    with open(label_json_path, 'r') as file:\n",
    "        labels = json.load(file)\n",
    "\n",
    "    # Iterate over each data entry in the loaded labels\n",
    "    for data_entry in labels:\n",
    "        # Process each caption using the s_split function\n",
    "        for caption in data_entry['caption']:\n",
    "            indexed_caption = s_split(caption, word_dict, w2i)\n",
    "            annotated_captions.append((data_entry['id'], indexed_caption))\n",
    "\n",
    "    return annotated_captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ada08f2f-dc64-4386-8c48-523061f3c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avi(files_dir):\n",
    "    avi_data = {}\n",
    "    training_feats = 'data/' + files_dir\n",
    "    files = os.listdir(training_feats)\n",
    "    i = 0\n",
    "    for file in files:\n",
    "        print(\"Loading file no:-  \" + str(i))\n",
    "        i+=1\n",
    "        value = np.load(os.path.join(training_feats, file))\n",
    "        avi_data[file.split('.npy')[0]] = value\n",
    "    return avi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afaf96dd-b1cc-4344-abdb-5a797d47b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(data):\n",
    "    # Sort the data by the length of captions in descending order\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    \n",
    "    # Unpack the data into separate lists of avi_data and captions\n",
    "    avi_data, captions = zip(*data)\n",
    "    \n",
    "    # Stack the avi_data into a tensor\n",
    "    avi_data = torch.stack(avi_data, 0)\n",
    "    \n",
    "    # Get the lengths of each caption\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    \n",
    "    # Initialize a zero tensor for targets with dimensions [batch_size, max_caption_length]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    \n",
    "    # Fill in the targets tensor with caption indices\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = torch.tensor(cap[:end], dtype=torch.long)\n",
    "    \n",
    "    # Return the avi_data, targets tensor, and lengths of each caption\n",
    "    return avi_data, targets, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f90ee9c2-4de1-4101-a26a-7b9fa4116a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingData(Dataset):\n",
    "    def __init__(self, label_file, files_dir, word_dict, w2i):\n",
    "        self.label_file = label_file\n",
    "        self.files_dir = files_dir\n",
    "        self.word_dict = word_dict\n",
    "        self.avi = avi(label_file)\n",
    "        self.w2i = w2i\n",
    "        self.data_pair = annotate(files_dir, word_dict, w2i)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Returns the number of items in the dataset\n",
    "        return len(self.data_pair)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #Check\n",
    "        assert idx < self.__len__(), \"Index out of range\"\n",
    "        \n",
    "        avi_file_name, sentence = self.data_pair[idx]\n",
    "        # Get the video data as a tensor and add some noise\n",
    "        video_data = torch.Tensor(self.avi[avi_file_name])\n",
    "        video_data += torch.Tensor(video_data.size()).random_(0, 2000) / 10000.\n",
    "        # Convert sentence to tensor\n",
    "        caption_tensor = torch.Tensor(sentence).long()\n",
    "        \n",
    "        return video_data, caption_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21e21f82-c697-41d5-a386-99fcbca275a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestingData(Dataset):\n",
    "    def __init__(self, test_data_path):\n",
    "        self.avi = []\n",
    "        files = os.listdir(test_data_path)\n",
    "        for file in files:\n",
    "            key = file.split('.npy')[0]\n",
    "            value = np.load(os.path.join(test_data_path, file))\n",
    "            self.avi.append([key, value])\n",
    "            \n",
    "    def __len__(self):\n",
    "        # Returns the number of items in the dataset\n",
    "        return len(self.avi)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Returns the idx-th item of the dataset\n",
    "        assert idx < len(self), \"Index out of range\"\n",
    "        return self.avi[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff47b11f-fb0f-4e8a-a1ae-311c3026489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_layer1 = nn.Linear(2*hidden_size, hidden_size)\n",
    "        self.attention_layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.attention_layer3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.attention_layer4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.compute_weight = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden_state, encoder_outputs):\n",
    "        batch_size, seq_len, hidden_dim = encoder_outputs.size()\n",
    "        hidden_state_expanded = hidden_state.view(batch_size, 1, hidden_dim).repeat(1, seq_len, 1)\n",
    "        combined_inputs = torch.cat((encoder_outputs, hidden_state_expanded), 2).view(-1, 2*self.hidden_size)\n",
    "\n",
    "        attn_hidden = self.attention_layer1(combined_inputs)\n",
    "        attn_hidden = self.attention_layer2(attn_hidden)\n",
    "        attn_hidden = self.attention_layer3(attn_hidden)\n",
    "        attn_hidden = self.attention_layer4(attn_hidden)\n",
    "        weights = self.compute_weight(attn_hidden)\n",
    "        weights = weights.view(batch_size, seq_len)\n",
    "        weights_normalized = F.softmax(weights, dim=1)\n",
    "        context_vector = torch.bmm(weights_normalized.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        \n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89f30cfd-2b9b-4fe1-aaba-1b8e54ef9bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        \n",
    "        self.Embedding = nn.Linear(4096, 512)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.lstm = nn.LSTM(512, 512, batch_first=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, seq_len, feat_n = input.size()    \n",
    "        input = input.view(-1, feat_n)\n",
    "        input = self.Embedding(input)\n",
    "        input = self.dropout(input)\n",
    "        input = input.view(batch_size, seq_len, 512)\n",
    "\n",
    "        output, t = self.lstm(input)\n",
    "        hidden_state, context = t[0], t[1]\n",
    "        return output, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3795131f-ff8a-4099-969e-48b97f60df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, vocab_size, word_dim, dropout_percentage=0.33):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = 512\n",
    "        self.output_size = output_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_dim = word_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, 1024)\n",
    "        self.dropout = nn.Dropout(0.33)\n",
    "        self.lstm = nn.LSTM(hidden_size+word_dim, hidden_size, batch_first=True)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.to_final_output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, encoder_last_hidden_state, encoder_output, targets=None, mode='train', tr_steps=None):\n",
    "        _, batch_size, _ = encoder_last_hidden_state.size()\n",
    "        \n",
    "        decoder_current_hidden_state = None if encoder_last_hidden_state is None else encoder_last_hidden_state\n",
    "        decoder_cxt = torch.zeros(decoder_current_hidden_state.size()).cuda()\n",
    "\n",
    "        decoder_current_input_word = Variable(torch.ones(batch_size, 1)).long().cuda()\n",
    "        seq_logProb = []\n",
    "        seq_predictions = []\n",
    "\n",
    "        targets = self.embedding(targets)\n",
    "        _, seq_len, _ = targets.size()\n",
    "\n",
    "        for i in range(seq_len-1):\n",
    "            threshold = self.teacher_forcing_ratio(training_steps=tr_steps)\n",
    "            if random.uniform(0.05, 0.995) > threshold: # returns a random float value between 0.05 and 0.995\n",
    "                current_input_word = targets[:, i]  \n",
    "            else: \n",
    "                current_input_word = self.embedding(decoder_current_input_word).squeeze(1)\n",
    "\n",
    "            context = self.attention(decoder_current_hidden_state, encoder_output)\n",
    "            lstm_input = torch.cat([current_input_word, context], dim=1).unsqueeze(1)\n",
    "            lstm_output, t = self.lstm(lstm_input, (decoder_current_hidden_state,decoder_cxt))\n",
    "            decoder_current_hidden_state=t[0]\n",
    "            logprob = self.to_final_output(lstm_output.squeeze(1))\n",
    "            seq_logProb.append(logprob.unsqueeze(1))\n",
    "            decoder_current_input_word = logprob.unsqueeze(1).max(2)[1]\n",
    "\n",
    "        seq_logProb = torch.cat(seq_logProb, dim=1)\n",
    "        seq_predictions = seq_logProb.max(2)[1]\n",
    "        return seq_logProb, seq_predictions\n",
    "        \n",
    "    def infer(self, encoder_last_hidden_state, encoder_output):\n",
    "        _, batch_size, _ = encoder_last_hidden_state.size()\n",
    "        decoder_current_hidden_state = None if encoder_last_hidden_state is None else encoder_last_hidden_state\n",
    "        decoder_current_input_word = Variable(torch.ones(batch_size, 1)).long()\n",
    "        decoder_c= torch.zeros(decoder_current_hidden_state.size())\n",
    "        seq_logProb = []\n",
    "        seq_predictions = []\n",
    "        assumption_seq_len = 28\n",
    "        \n",
    "        for i in range(assumption_seq_len-1):\n",
    "            current_input_word = self.embedding(decoder_current_input_word).squeeze(1)\n",
    "            context = self.attention(decoder_current_hidden_state, encoder_output)\n",
    "            lstm_input = torch.cat([current_input_word, context], dim=1).unsqueeze(1)\n",
    "            lstm_output,  t = self.lstm(lstm_input, (decoder_current_hidden_state,decoder_c))\n",
    "            decoder_current_hidden_state=t[0]\n",
    "            logprob = self.to_final_output(lstm_output.squeeze(1))\n",
    "            seq_logProb.append(logprob.unsqueeze(1))\n",
    "            decoder_current_input_word = logprob.unsqueeze(1).max(2)[1]\n",
    "\n",
    "        seq_logProb = torch.cat(seq_logProb, dim=1)\n",
    "        seq_predictions = seq_logProb.max(2)[1]\n",
    "        return seq_logProb, seq_predictions\n",
    "\n",
    "    def teacher_forcing_ratio(self, training_steps):\n",
    "        return (expit(training_steps/20 +0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb2d12cf-3810-4b0c-bc1b-fc671fdb9ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODELS(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(MODELS, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, avi_feat, mode, target_sentences=None, tr_steps=None):\n",
    "        encoder_outputs, encoder_last_hidden_state = self.encoder(avi_feat)\n",
    "        if mode == 'train':\n",
    "            seq_logProb, seq_predictions = self.decoder(encoder_last_hidden_state = encoder_last_hidden_state, encoder_output = encoder_outputs,\n",
    "                targets = target_sentences, mode = mode, tr_steps=tr_steps)\n",
    "        elif mode == 'inference':\n",
    "            seq_logProb, seq_predictions = self.decoder.infer(encoder_last_hidden_state=encoder_last_hidden_state, encoder_output=encoder_outputs)\n",
    "        return seq_logProb, seq_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47cd3e69-2cd4-488e-9456-5dba7a2e2fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(loss_fn, predictions, targets, lengths):\n",
    "    batch_size = len(predictions)\n",
    "    concatenated_predictions = None\n",
    "    concatenated_targets = None\n",
    "    is_first_batch = True \n",
    "\n",
    "    for i in range(batch_size):\n",
    "        current_prediction = predictions[i]\n",
    "        current_target = targets[i]\n",
    "        current_length = lengths[i] - 1 \n",
    "\n",
    "        # Trim the sequences based on the actual length\n",
    "        trimmed_prediction = current_prediction[:current_length]\n",
    "        trimmed_target = current_target[:current_length]\n",
    "\n",
    "        # Initialize or concatenate the sequences\n",
    "        if is_first_batch:\n",
    "            concatenated_predictions = trimmed_prediction\n",
    "            concatenated_targets = trimmed_target\n",
    "            is_first_batch = False \n",
    "        else:\n",
    "            concatenated_predictions = torch.cat((concatenated_predictions, trimmed_prediction), dim=0)\n",
    "            concatenated_targets = torch.cat((concatenated_targets, trimmed_target), dim=0)\n",
    "\n",
    "    # Compute the loss on the concatenated sequences\n",
    "    total_loss = loss_fn(concatenated_predictions, concatenated_targets)\n",
    "    avg_loss = total_loss / batch_size  #average loss\n",
    "\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a176ea9f-771e-4a02-8c0b-f96f5509ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def minibatch(data):\n",
    "    # Sort the input data by the length of the captions in descending order\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    video_data, captions = zip(*data)\n",
    "    video_tensor = torch.stack(video_data, 0)\n",
    "    \n",
    "    caption_lengths = [len(cap) for cap in captions]\n",
    "    \n",
    "    # Initialize a zero tensor for all captions based on the longest caption\n",
    "    target_tensor = torch.zeros(len(captions), max(caption_lengths)).long()\n",
    "    \n",
    "    # Fill in the target tensor with actual captions, padding the rest\n",
    "    for index, caption in enumerate(captions):\n",
    "        end = caption_lengths[index]\n",
    "        target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
    "    \n",
    "    return video_tensor, target_tensor, caption_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49280678-b235-4293-b0c3-b1a9467231be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, epoch, loss_fn, parameters, optimizer, train_loader):\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        avi_feats, ground_truths, lengths = batch\n",
    "        avi_feats, ground_truths = Variable(avi_feats).cuda(), Variable(ground_truths).cuda()\n",
    "        # avi_feats, ground_truths = Variable(avi_feats), Variable(ground_truths)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        seq_logProb, seq_predictions = model(avi_feats, target_sentences = ground_truths, mode = 'train', tr_steps = epoch)\n",
    "        ground_truths = ground_truths[:, 1:]  \n",
    "        loss = calculate_loss(loss_fn, seq_logProb, ground_truths, lengths)\n",
    "        print('Batch - ', batch_idx, ' Loss - ', loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    loss = loss.item()\n",
    "    return loss\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94ba5a6b-0fdf-4fa2-a262-d0dc196b152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, i2w):\n",
    "    model.eval()\n",
    "    _ss = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "     \n",
    "        id, avi_feats = batch\n",
    "        avi_feats = avi_feats.cuda()\n",
    "        id, avi_feats = id, Variable(avi_feats).float()\n",
    "        \n",
    "        seq_logProb, seq_predictions = model(avi_feats, mode='inference')\n",
    "        test_predictions = seq_predictions\n",
    "        \n",
    "        result = [[i2w[x.item()] if i2w[x.item()] != '<UNK>' else 'something' for x in s] for s in test_predictions]\n",
    "        result = [' '.join(s).split('<EOS>')[0] for s in result]\n",
    "        rr = zip(id, result)\n",
    "        for r in rr:\n",
    "            _ss.append(r)\n",
    "    return _ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edcf6efa-42a6-4670-93c6-9b139cf7ea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file no:-  0\n",
      "Loading file no:-  1\n",
      "Loading file no:-  2\n",
      "Loading file no:-  3\n",
      "Loading file no:-  4\n",
      "Loading file no:-  5\n",
      "Loading file no:-  6\n",
      "Loading file no:-  7\n",
      "Loading file no:-  8\n",
      "Loading file no:-  9\n",
      "Loading file no:-  10\n",
      "Loading file no:-  11\n",
      "Loading file no:-  12\n",
      "Loading file no:-  13\n",
      "Loading file no:-  14\n",
      "Loading file no:-  15\n",
      "Loading file no:-  16\n",
      "Loading file no:-  17\n",
      "Loading file no:-  18\n",
      "Loading file no:-  19\n",
      "Loading file no:-  20\n",
      "Loading file no:-  21\n",
      "Loading file no:-  22\n",
      "Loading file no:-  23\n",
      "Loading file no:-  24\n",
      "Loading file no:-  25\n",
      "Loading file no:-  26\n",
      "Loading file no:-  27\n",
      "Loading file no:-  28\n",
      "Loading file no:-  29\n",
      "Loading file no:-  30\n",
      "Loading file no:-  31\n",
      "Loading file no:-  32\n",
      "Loading file no:-  33\n",
      "Loading file no:-  34\n",
      "Loading file no:-  35\n",
      "Loading file no:-  36\n",
      "Loading file no:-  37\n",
      "Loading file no:-  38\n",
      "Loading file no:-  39\n",
      "Loading file no:-  40\n",
      "Loading file no:-  41\n",
      "Loading file no:-  42\n",
      "Loading file no:-  43\n",
      "Loading file no:-  44\n",
      "Loading file no:-  45\n",
      "Loading file no:-  46\n",
      "Loading file no:-  47\n",
      "Loading file no:-  48\n",
      "Loading file no:-  49\n",
      "Loading file no:-  50\n",
      "Loading file no:-  51\n",
      "Loading file no:-  52\n",
      "Loading file no:-  53\n",
      "Loading file no:-  54\n",
      "Loading file no:-  55\n",
      "Loading file no:-  56\n",
      "Loading file no:-  57\n",
      "Loading file no:-  58\n",
      "Loading file no:-  59\n",
      "Loading file no:-  60\n",
      "Loading file no:-  61\n",
      "Loading file no:-  62\n",
      "Loading file no:-  63\n",
      "Loading file no:-  64\n",
      "Loading file no:-  65\n",
      "Loading file no:-  66\n",
      "Loading file no:-  67\n",
      "Loading file no:-  68\n",
      "Loading file no:-  69\n",
      "Loading file no:-  70\n",
      "Loading file no:-  71\n",
      "Loading file no:-  72\n",
      "Loading file no:-  73\n",
      "Loading file no:-  74\n",
      "Loading file no:-  75\n",
      "Loading file no:-  76\n",
      "Loading file no:-  77\n",
      "Loading file no:-  78\n",
      "Loading file no:-  79\n",
      "Loading file no:-  80\n",
      "Loading file no:-  81\n",
      "Loading file no:-  82\n",
      "Loading file no:-  83\n",
      "Loading file no:-  84\n",
      "Loading file no:-  85\n",
      "Loading file no:-  86\n",
      "Loading file no:-  87\n",
      "Loading file no:-  88\n",
      "Loading file no:-  89\n",
      "Loading file no:-  90\n",
      "Loading file no:-  91\n",
      "Loading file no:-  92\n",
      "Loading file no:-  93\n",
      "Loading file no:-  94\n",
      "Loading file no:-  95\n",
      "Loading file no:-  96\n",
      "Loading file no:-  97\n",
      "Loading file no:-  98\n",
      "Loading file no:-  99\n",
      "Loading file no:-  100\n",
      "Loading file no:-  101\n",
      "Loading file no:-  102\n",
      "Loading file no:-  103\n",
      "Loading file no:-  104\n",
      "Loading file no:-  105\n",
      "Loading file no:-  106\n",
      "Loading file no:-  107\n",
      "Loading file no:-  108\n",
      "Loading file no:-  109\n",
      "Loading file no:-  110\n",
      "Loading file no:-  111\n",
      "Loading file no:-  112\n",
      "Loading file no:-  113\n",
      "Loading file no:-  114\n",
      "Loading file no:-  115\n",
      "Loading file no:-  116\n",
      "Loading file no:-  117\n",
      "Loading file no:-  118\n",
      "Loading file no:-  119\n",
      "Loading file no:-  120\n",
      "Loading file no:-  121\n",
      "Loading file no:-  122\n",
      "Loading file no:-  123\n",
      "Loading file no:-  124\n",
      "Loading file no:-  125\n",
      "Loading file no:-  126\n",
      "Loading file no:-  127\n",
      "Loading file no:-  128\n",
      "Loading file no:-  129\n",
      "Loading file no:-  130\n",
      "Loading file no:-  131\n",
      "Loading file no:-  132\n",
      "Loading file no:-  133\n",
      "Loading file no:-  134\n",
      "Loading file no:-  135\n",
      "Loading file no:-  136\n",
      "Loading file no:-  137\n",
      "Loading file no:-  138\n",
      "Loading file no:-  139\n",
      "Loading file no:-  140\n",
      "Loading file no:-  141\n",
      "Loading file no:-  142\n",
      "Loading file no:-  143\n",
      "Loading file no:-  144\n",
      "Loading file no:-  145\n",
      "Loading file no:-  146\n",
      "Loading file no:-  147\n",
      "Loading file no:-  148\n",
      "Loading file no:-  149\n",
      "Loading file no:-  150\n",
      "Loading file no:-  151\n",
      "Loading file no:-  152\n",
      "Loading file no:-  153\n",
      "Loading file no:-  154\n",
      "Loading file no:-  155\n",
      "Loading file no:-  156\n",
      "Loading file no:-  157\n",
      "Loading file no:-  158\n",
      "Loading file no:-  159\n",
      "Loading file no:-  160\n",
      "Loading file no:-  161\n",
      "Loading file no:-  162\n",
      "Loading file no:-  163\n",
      "Loading file no:-  164\n",
      "Loading file no:-  165\n",
      "Loading file no:-  166\n",
      "Loading file no:-  167\n",
      "Loading file no:-  168\n",
      "Loading file no:-  169\n",
      "Loading file no:-  170\n",
      "Loading file no:-  171\n",
      "Loading file no:-  172\n",
      "Loading file no:-  173\n",
      "Loading file no:-  174\n",
      "Loading file no:-  175\n",
      "Loading file no:-  176\n",
      "Loading file no:-  177\n",
      "Loading file no:-  178\n",
      "Loading file no:-  179\n",
      "Loading file no:-  180\n",
      "Loading file no:-  181\n",
      "Loading file no:-  182\n",
      "Loading file no:-  183\n",
      "Loading file no:-  184\n",
      "Loading file no:-  185\n",
      "Loading file no:-  186\n",
      "Loading file no:-  187\n",
      "Loading file no:-  188\n",
      "Loading file no:-  189\n",
      "Loading file no:-  190\n",
      "Loading file no:-  191\n",
      "Loading file no:-  192\n",
      "Loading file no:-  193\n",
      "Loading file no:-  194\n",
      "Loading file no:-  195\n",
      "Loading file no:-  196\n",
      "Loading file no:-  197\n",
      "Loading file no:-  198\n",
      "Loading file no:-  199\n",
      "Loading file no:-  200\n",
      "Loading file no:-  201\n",
      "Loading file no:-  202\n",
      "Loading file no:-  203\n",
      "Loading file no:-  204\n",
      "Loading file no:-  205\n",
      "Loading file no:-  206\n",
      "Loading file no:-  207\n",
      "Loading file no:-  208\n",
      "Loading file no:-  209\n",
      "Loading file no:-  210\n",
      "Loading file no:-  211\n",
      "Loading file no:-  212\n",
      "Loading file no:-  213\n",
      "Loading file no:-  214\n",
      "Loading file no:-  215\n",
      "Loading file no:-  216\n",
      "Loading file no:-  217\n",
      "Loading file no:-  218\n",
      "Loading file no:-  219\n",
      "Loading file no:-  220\n",
      "Loading file no:-  221\n",
      "Loading file no:-  222\n",
      "Loading file no:-  223\n",
      "Loading file no:-  224\n",
      "Loading file no:-  225\n",
      "Loading file no:-  226\n",
      "Loading file no:-  227\n",
      "Loading file no:-  228\n",
      "Loading file no:-  229\n",
      "Loading file no:-  230\n",
      "Loading file no:-  231\n",
      "Loading file no:-  232\n",
      "Loading file no:-  233\n",
      "Loading file no:-  234\n",
      "Loading file no:-  235\n",
      "Loading file no:-  236\n",
      "Loading file no:-  237\n",
      "Loading file no:-  238\n",
      "Loading file no:-  239\n",
      "Loading file no:-  240\n",
      "Loading file no:-  241\n",
      "Loading file no:-  242\n",
      "Loading file no:-  243\n",
      "Loading file no:-  244\n",
      "Loading file no:-  245\n",
      "Loading file no:-  246\n",
      "Loading file no:-  247\n",
      "Loading file no:-  248\n",
      "Loading file no:-  249\n",
      "Loading file no:-  250\n",
      "Loading file no:-  251\n",
      "Loading file no:-  252\n",
      "Loading file no:-  253\n",
      "Loading file no:-  254\n",
      "Loading file no:-  255\n",
      "Loading file no:-  256\n",
      "Loading file no:-  257\n",
      "Loading file no:-  258\n",
      "Loading file no:-  259\n",
      "Loading file no:-  260\n",
      "Loading file no:-  261\n",
      "Loading file no:-  262\n",
      "Loading file no:-  263\n",
      "Loading file no:-  264\n",
      "Loading file no:-  265\n",
      "Loading file no:-  266\n",
      "Loading file no:-  267\n",
      "Loading file no:-  268\n",
      "Loading file no:-  269\n",
      "Loading file no:-  270\n",
      "Loading file no:-  271\n",
      "Loading file no:-  272\n",
      "Loading file no:-  273\n",
      "Loading file no:-  274\n",
      "Loading file no:-  275\n",
      "Loading file no:-  276\n",
      "Loading file no:-  277\n",
      "Loading file no:-  278\n",
      "Loading file no:-  279\n",
      "Loading file no:-  280\n",
      "Loading file no:-  281\n",
      "Loading file no:-  282\n",
      "Loading file no:-  283\n",
      "Loading file no:-  284\n",
      "Loading file no:-  285\n",
      "Loading file no:-  286\n",
      "Loading file no:-  287\n",
      "Loading file no:-  288\n",
      "Loading file no:-  289\n",
      "Loading file no:-  290\n",
      "Loading file no:-  291\n",
      "Loading file no:-  292\n",
      "Loading file no:-  293\n",
      "Loading file no:-  294\n",
      "Loading file no:-  295\n",
      "Loading file no:-  296\n",
      "Loading file no:-  297\n",
      "Loading file no:-  298\n",
      "Loading file no:-  299\n",
      "Loading file no:-  300\n",
      "Loading file no:-  301\n",
      "Loading file no:-  302\n",
      "Loading file no:-  303\n",
      "Loading file no:-  304\n",
      "Loading file no:-  305\n",
      "Loading file no:-  306\n",
      "Loading file no:-  307\n",
      "Loading file no:-  308\n",
      "Loading file no:-  309\n",
      "Loading file no:-  310\n",
      "Loading file no:-  311\n",
      "Loading file no:-  312\n",
      "Loading file no:-  313\n",
      "Loading file no:-  314\n",
      "Loading file no:-  315\n",
      "Loading file no:-  316\n",
      "Loading file no:-  317\n",
      "Loading file no:-  318\n",
      "Loading file no:-  319\n",
      "Loading file no:-  320\n",
      "Loading file no:-  321\n",
      "Loading file no:-  322\n",
      "Loading file no:-  323\n",
      "Loading file no:-  324\n",
      "Loading file no:-  325\n",
      "Loading file no:-  326\n",
      "Loading file no:-  327\n",
      "Loading file no:-  328\n",
      "Loading file no:-  329\n",
      "Loading file no:-  330\n",
      "Loading file no:-  331\n",
      "Loading file no:-  332\n",
      "Loading file no:-  333\n",
      "Loading file no:-  334\n",
      "Loading file no:-  335\n",
      "Loading file no:-  336\n",
      "Loading file no:-  337\n",
      "Loading file no:-  338\n",
      "Loading file no:-  339\n",
      "Loading file no:-  340\n",
      "Loading file no:-  341\n",
      "Loading file no:-  342\n",
      "Loading file no:-  343\n",
      "Loading file no:-  344\n",
      "Loading file no:-  345\n",
      "Loading file no:-  346\n",
      "Loading file no:-  347\n",
      "Loading file no:-  348\n",
      "Loading file no:-  349\n",
      "Loading file no:-  350\n",
      "Loading file no:-  351\n",
      "Loading file no:-  352\n",
      "Loading file no:-  353\n",
      "Loading file no:-  354\n",
      "Loading file no:-  355\n",
      "Loading file no:-  356\n",
      "Loading file no:-  357\n",
      "Loading file no:-  358\n",
      "Loading file no:-  359\n",
      "Loading file no:-  360\n",
      "Loading file no:-  361\n",
      "Loading file no:-  362\n",
      "Loading file no:-  363\n",
      "Loading file no:-  364\n",
      "Loading file no:-  365\n",
      "Loading file no:-  366\n",
      "Loading file no:-  367\n",
      "Loading file no:-  368\n",
      "Loading file no:-  369\n",
      "Loading file no:-  370\n",
      "Loading file no:-  371\n",
      "Loading file no:-  372\n",
      "Loading file no:-  373\n",
      "Loading file no:-  374\n",
      "Loading file no:-  375\n",
      "Loading file no:-  376\n",
      "Loading file no:-  377\n",
      "Loading file no:-  378\n",
      "Loading file no:-  379\n",
      "Loading file no:-  380\n",
      "Loading file no:-  381\n",
      "Loading file no:-  382\n",
      "Loading file no:-  383\n",
      "Loading file no:-  384\n",
      "Loading file no:-  385\n",
      "Loading file no:-  386\n",
      "Loading file no:-  387\n",
      "Loading file no:-  388\n",
      "Loading file no:-  389\n",
      "Loading file no:-  390\n",
      "Loading file no:-  391\n",
      "Loading file no:-  392\n",
      "Loading file no:-  393\n",
      "Loading file no:-  394\n",
      "Loading file no:-  395\n",
      "Loading file no:-  396\n",
      "Loading file no:-  397\n",
      "Loading file no:-  398\n",
      "Loading file no:-  399\n",
      "Loading file no:-  400\n",
      "Loading file no:-  401\n",
      "Loading file no:-  402\n",
      "Loading file no:-  403\n",
      "Loading file no:-  404\n",
      "Loading file no:-  405\n",
      "Loading file no:-  406\n",
      "Loading file no:-  407\n",
      "Loading file no:-  408\n",
      "Loading file no:-  409\n",
      "Loading file no:-  410\n",
      "Loading file no:-  411\n",
      "Loading file no:-  412\n",
      "Loading file no:-  413\n",
      "Loading file no:-  414\n",
      "Loading file no:-  415\n",
      "Loading file no:-  416\n",
      "Loading file no:-  417\n",
      "Loading file no:-  418\n",
      "Loading file no:-  419\n",
      "Loading file no:-  420\n",
      "Loading file no:-  421\n",
      "Loading file no:-  422\n",
      "Loading file no:-  423\n",
      "Loading file no:-  424\n",
      "Loading file no:-  425\n",
      "Loading file no:-  426\n",
      "Loading file no:-  427\n",
      "Loading file no:-  428\n",
      "Loading file no:-  429\n",
      "Loading file no:-  430\n",
      "Loading file no:-  431\n",
      "Loading file no:-  432\n",
      "Loading file no:-  433\n",
      "Loading file no:-  434\n",
      "Loading file no:-  435\n",
      "Loading file no:-  436\n",
      "Loading file no:-  437\n",
      "Loading file no:-  438\n",
      "Loading file no:-  439\n",
      "Loading file no:-  440\n",
      "Loading file no:-  441\n",
      "Loading file no:-  442\n",
      "Loading file no:-  443\n",
      "Loading file no:-  444\n",
      "Loading file no:-  445\n",
      "Loading file no:-  446\n",
      "Loading file no:-  447\n",
      "Loading file no:-  448\n",
      "Loading file no:-  449\n",
      "Loading file no:-  450\n",
      "Loading file no:-  451\n",
      "Loading file no:-  452\n",
      "Loading file no:-  453\n",
      "Loading file no:-  454\n",
      "Loading file no:-  455\n",
      "Loading file no:-  456\n",
      "Loading file no:-  457\n",
      "Loading file no:-  458\n",
      "Loading file no:-  459\n",
      "Loading file no:-  460\n",
      "Loading file no:-  461\n",
      "Loading file no:-  462\n",
      "Loading file no:-  463\n",
      "Loading file no:-  464\n",
      "Loading file no:-  465\n",
      "Loading file no:-  466\n",
      "Loading file no:-  467\n",
      "Loading file no:-  468\n",
      "Loading file no:-  469\n",
      "Loading file no:-  470\n",
      "Loading file no:-  471\n",
      "Loading file no:-  472\n",
      "Loading file no:-  473\n",
      "Loading file no:-  474\n",
      "Loading file no:-  475\n",
      "Loading file no:-  476\n",
      "Loading file no:-  477\n",
      "Loading file no:-  478\n",
      "Loading file no:-  479\n",
      "Loading file no:-  480\n",
      "Loading file no:-  481\n",
      "Loading file no:-  482\n",
      "Loading file no:-  483\n",
      "Loading file no:-  484\n",
      "Loading file no:-  485\n",
      "Loading file no:-  486\n",
      "Loading file no:-  487\n",
      "Loading file no:-  488\n",
      "Loading file no:-  489\n",
      "Loading file no:-  490\n",
      "Loading file no:-  491\n",
      "Loading file no:-  492\n",
      "Loading file no:-  493\n",
      "Loading file no:-  494\n",
      "Loading file no:-  495\n",
      "Loading file no:-  496\n",
      "Loading file no:-  497\n",
      "Loading file no:-  498\n",
      "Loading file no:-  499\n",
      "Loading file no:-  500\n",
      "Loading file no:-  501\n",
      "Loading file no:-  502\n",
      "Loading file no:-  503\n",
      "Loading file no:-  504\n",
      "Loading file no:-  505\n",
      "Loading file no:-  506\n",
      "Loading file no:-  507\n",
      "Loading file no:-  508\n",
      "Loading file no:-  509\n",
      "Loading file no:-  510\n",
      "Loading file no:-  511\n",
      "Loading file no:-  512\n",
      "Loading file no:-  513\n",
      "Loading file no:-  514\n",
      "Loading file no:-  515\n",
      "Loading file no:-  516\n",
      "Loading file no:-  517\n",
      "Loading file no:-  518\n",
      "Loading file no:-  519\n",
      "Loading file no:-  520\n",
      "Loading file no:-  521\n",
      "Loading file no:-  522\n",
      "Loading file no:-  523\n",
      "Loading file no:-  524\n",
      "Loading file no:-  525\n",
      "Loading file no:-  526\n",
      "Loading file no:-  527\n",
      "Loading file no:-  528\n",
      "Loading file no:-  529\n",
      "Loading file no:-  530\n",
      "Loading file no:-  531\n",
      "Loading file no:-  532\n",
      "Loading file no:-  533\n",
      "Loading file no:-  534\n",
      "Loading file no:-  535\n",
      "Loading file no:-  536\n",
      "Loading file no:-  537\n",
      "Loading file no:-  538\n",
      "Loading file no:-  539\n",
      "Loading file no:-  540\n",
      "Loading file no:-  541\n",
      "Loading file no:-  542\n",
      "Loading file no:-  543\n",
      "Loading file no:-  544\n",
      "Loading file no:-  545\n",
      "Loading file no:-  546\n",
      "Loading file no:-  547\n",
      "Loading file no:-  548\n",
      "Loading file no:-  549\n",
      "Loading file no:-  550\n",
      "Loading file no:-  551\n",
      "Loading file no:-  552\n",
      "Loading file no:-  553\n",
      "Loading file no:-  554\n",
      "Loading file no:-  555\n",
      "Loading file no:-  556\n",
      "Loading file no:-  557\n",
      "Loading file no:-  558\n",
      "Loading file no:-  559\n",
      "Loading file no:-  560\n",
      "Loading file no:-  561\n",
      "Loading file no:-  562\n",
      "Loading file no:-  563\n",
      "Loading file no:-  564\n",
      "Loading file no:-  565\n",
      "Loading file no:-  566\n",
      "Loading file no:-  567\n",
      "Loading file no:-  568\n",
      "Loading file no:-  569\n",
      "Loading file no:-  570\n",
      "Loading file no:-  571\n",
      "Loading file no:-  572\n",
      "Loading file no:-  573\n",
      "Loading file no:-  574\n",
      "Loading file no:-  575\n",
      "Loading file no:-  576\n",
      "Loading file no:-  577\n",
      "Loading file no:-  578\n",
      "Loading file no:-  579\n",
      "Loading file no:-  580\n",
      "Loading file no:-  581\n",
      "Loading file no:-  582\n",
      "Loading file no:-  583\n",
      "Loading file no:-  584\n",
      "Loading file no:-  585\n",
      "Loading file no:-  586\n",
      "Loading file no:-  587\n",
      "Loading file no:-  588\n",
      "Loading file no:-  589\n",
      "Loading file no:-  590\n",
      "Loading file no:-  591\n",
      "Loading file no:-  592\n",
      "Loading file no:-  593\n",
      "Loading file no:-  594\n",
      "Loading file no:-  595\n",
      "Loading file no:-  596\n",
      "Loading file no:-  597\n",
      "Loading file no:-  598\n",
      "Loading file no:-  599\n",
      "Loading file no:-  600\n",
      "Loading file no:-  601\n",
      "Loading file no:-  602\n",
      "Loading file no:-  603\n",
      "Loading file no:-  604\n",
      "Loading file no:-  605\n",
      "Loading file no:-  606\n",
      "Loading file no:-  607\n",
      "Loading file no:-  608\n",
      "Loading file no:-  609\n",
      "Loading file no:-  610\n",
      "Loading file no:-  611\n",
      "Loading file no:-  612\n",
      "Loading file no:-  613\n",
      "Loading file no:-  614\n",
      "Loading file no:-  615\n",
      "Loading file no:-  616\n",
      "Loading file no:-  617\n",
      "Loading file no:-  618\n",
      "Loading file no:-  619\n",
      "Loading file no:-  620\n",
      "Loading file no:-  621\n",
      "Loading file no:-  622\n",
      "Loading file no:-  623\n",
      "Loading file no:-  624\n",
      "Loading file no:-  625\n",
      "Loading file no:-  626\n",
      "Loading file no:-  627\n",
      "Loading file no:-  628\n",
      "Loading file no:-  629\n",
      "Loading file no:-  630\n",
      "Loading file no:-  631\n",
      "Loading file no:-  632\n",
      "Loading file no:-  633\n",
      "Loading file no:-  634\n",
      "Loading file no:-  635\n",
      "Loading file no:-  636\n",
      "Loading file no:-  637\n",
      "Loading file no:-  638\n",
      "Loading file no:-  639\n",
      "Loading file no:-  640\n",
      "Loading file no:-  641\n",
      "Loading file no:-  642\n",
      "Loading file no:-  643\n",
      "Loading file no:-  644\n",
      "Loading file no:-  645\n",
      "Loading file no:-  646\n",
      "Loading file no:-  647\n",
      "Loading file no:-  648\n",
      "Loading file no:-  649\n",
      "Loading file no:-  650\n",
      "Loading file no:-  651\n",
      "Loading file no:-  652\n",
      "Loading file no:-  653\n",
      "Loading file no:-  654\n",
      "Loading file no:-  655\n",
      "Loading file no:-  656\n",
      "Loading file no:-  657\n",
      "Loading file no:-  658\n",
      "Loading file no:-  659\n",
      "Loading file no:-  660\n",
      "Loading file no:-  661\n",
      "Loading file no:-  662\n",
      "Loading file no:-  663\n",
      "Loading file no:-  664\n",
      "Loading file no:-  665\n",
      "Loading file no:-  666\n",
      "Loading file no:-  667\n",
      "Loading file no:-  668\n",
      "Loading file no:-  669\n",
      "Loading file no:-  670\n",
      "Loading file no:-  671\n",
      "Loading file no:-  672\n",
      "Loading file no:-  673\n",
      "Loading file no:-  674\n",
      "Loading file no:-  675\n",
      "Loading file no:-  676\n",
      "Loading file no:-  677\n",
      "Loading file no:-  678\n",
      "Loading file no:-  679\n",
      "Loading file no:-  680\n",
      "Loading file no:-  681\n",
      "Loading file no:-  682\n",
      "Loading file no:-  683\n",
      "Loading file no:-  684\n",
      "Loading file no:-  685\n",
      "Loading file no:-  686\n",
      "Loading file no:-  687\n",
      "Loading file no:-  688\n",
      "Loading file no:-  689\n",
      "Loading file no:-  690\n",
      "Loading file no:-  691\n",
      "Loading file no:-  692\n",
      "Loading file no:-  693\n",
      "Loading file no:-  694\n",
      "Loading file no:-  695\n",
      "Loading file no:-  696\n",
      "Loading file no:-  697\n",
      "Loading file no:-  698\n",
      "Loading file no:-  699\n",
      "Loading file no:-  700\n",
      "Loading file no:-  701\n",
      "Loading file no:-  702\n",
      "Loading file no:-  703\n",
      "Loading file no:-  704\n",
      "Loading file no:-  705\n",
      "Loading file no:-  706\n",
      "Loading file no:-  707\n",
      "Loading file no:-  708\n",
      "Loading file no:-  709\n",
      "Loading file no:-  710\n",
      "Loading file no:-  711\n",
      "Loading file no:-  712\n",
      "Loading file no:-  713\n",
      "Loading file no:-  714\n",
      "Loading file no:-  715\n",
      "Loading file no:-  716\n",
      "Loading file no:-  717\n",
      "Loading file no:-  718\n",
      "Loading file no:-  719\n",
      "Loading file no:-  720\n",
      "Loading file no:-  721\n",
      "Loading file no:-  722\n",
      "Loading file no:-  723\n",
      "Loading file no:-  724\n",
      "Loading file no:-  725\n",
      "Loading file no:-  726\n",
      "Loading file no:-  727\n",
      "Loading file no:-  728\n",
      "Loading file no:-  729\n",
      "Loading file no:-  730\n",
      "Loading file no:-  731\n",
      "Loading file no:-  732\n",
      "Loading file no:-  733\n",
      "Loading file no:-  734\n",
      "Loading file no:-  735\n",
      "Loading file no:-  736\n",
      "Loading file no:-  737\n",
      "Loading file no:-  738\n",
      "Loading file no:-  739\n",
      "Loading file no:-  740\n",
      "Loading file no:-  741\n",
      "Loading file no:-  742\n",
      "Loading file no:-  743\n",
      "Loading file no:-  744\n",
      "Loading file no:-  745\n",
      "Loading file no:-  746\n",
      "Loading file no:-  747\n",
      "Loading file no:-  748\n",
      "Loading file no:-  749\n",
      "Loading file no:-  750\n",
      "Loading file no:-  751\n",
      "Loading file no:-  752\n",
      "Loading file no:-  753\n",
      "Loading file no:-  754\n",
      "Loading file no:-  755\n",
      "Loading file no:-  756\n",
      "Loading file no:-  757\n",
      "Loading file no:-  758\n",
      "Loading file no:-  759\n",
      "Loading file no:-  760\n",
      "Loading file no:-  761\n",
      "Loading file no:-  762\n",
      "Loading file no:-  763\n",
      "Loading file no:-  764\n",
      "Loading file no:-  765\n",
      "Loading file no:-  766\n",
      "Loading file no:-  767\n",
      "Loading file no:-  768\n",
      "Loading file no:-  769\n",
      "Loading file no:-  770\n",
      "Loading file no:-  771\n",
      "Loading file no:-  772\n",
      "Loading file no:-  773\n",
      "Loading file no:-  774\n",
      "Loading file no:-  775\n",
      "Loading file no:-  776\n",
      "Loading file no:-  777\n",
      "Loading file no:-  778\n",
      "Loading file no:-  779\n",
      "Loading file no:-  780\n",
      "Loading file no:-  781\n",
      "Loading file no:-  782\n",
      "Loading file no:-  783\n",
      "Loading file no:-  784\n",
      "Loading file no:-  785\n",
      "Loading file no:-  786\n",
      "Loading file no:-  787\n",
      "Loading file no:-  788\n",
      "Loading file no:-  789\n",
      "Loading file no:-  790\n",
      "Loading file no:-  791\n",
      "Loading file no:-  792\n",
      "Loading file no:-  793\n",
      "Loading file no:-  794\n",
      "Loading file no:-  795\n",
      "Loading file no:-  796\n",
      "Loading file no:-  797\n",
      "Loading file no:-  798\n",
      "Loading file no:-  799\n",
      "Loading file no:-  800\n",
      "Loading file no:-  801\n",
      "Loading file no:-  802\n",
      "Loading file no:-  803\n",
      "Loading file no:-  804\n",
      "Loading file no:-  805\n",
      "Loading file no:-  806\n",
      "Loading file no:-  807\n",
      "Loading file no:-  808\n",
      "Loading file no:-  809\n",
      "Loading file no:-  810\n",
      "Loading file no:-  811\n",
      "Loading file no:-  812\n",
      "Loading file no:-  813\n",
      "Loading file no:-  814\n",
      "Loading file no:-  815\n",
      "Loading file no:-  816\n",
      "Loading file no:-  817\n",
      "Loading file no:-  818\n",
      "Loading file no:-  819\n",
      "Loading file no:-  820\n",
      "Loading file no:-  821\n",
      "Loading file no:-  822\n",
      "Loading file no:-  823\n",
      "Loading file no:-  824\n",
      "Loading file no:-  825\n",
      "Loading file no:-  826\n",
      "Loading file no:-  827\n",
      "Loading file no:-  828\n",
      "Loading file no:-  829\n",
      "Loading file no:-  830\n",
      "Loading file no:-  831\n",
      "Loading file no:-  832\n",
      "Loading file no:-  833\n",
      "Loading file no:-  834\n",
      "Loading file no:-  835\n",
      "Loading file no:-  836\n",
      "Loading file no:-  837\n",
      "Loading file no:-  838\n",
      "Loading file no:-  839\n",
      "Loading file no:-  840\n",
      "Loading file no:-  841\n",
      "Loading file no:-  842\n",
      "Loading file no:-  843\n",
      "Loading file no:-  844\n",
      "Loading file no:-  845\n",
      "Loading file no:-  846\n",
      "Loading file no:-  847\n",
      "Loading file no:-  848\n",
      "Loading file no:-  849\n",
      "Loading file no:-  850\n",
      "Loading file no:-  851\n",
      "Loading file no:-  852\n",
      "Loading file no:-  853\n",
      "Loading file no:-  854\n",
      "Loading file no:-  855\n",
      "Loading file no:-  856\n",
      "Loading file no:-  857\n",
      "Loading file no:-  858\n",
      "Loading file no:-  859\n",
      "Loading file no:-  860\n",
      "Loading file no:-  861\n",
      "Loading file no:-  862\n",
      "Loading file no:-  863\n",
      "Loading file no:-  864\n",
      "Loading file no:-  865\n",
      "Loading file no:-  866\n",
      "Loading file no:-  867\n",
      "Loading file no:-  868\n",
      "Loading file no:-  869\n",
      "Loading file no:-  870\n",
      "Loading file no:-  871\n",
      "Loading file no:-  872\n",
      "Loading file no:-  873\n",
      "Loading file no:-  874\n",
      "Loading file no:-  875\n",
      "Loading file no:-  876\n",
      "Loading file no:-  877\n",
      "Loading file no:-  878\n",
      "Loading file no:-  879\n",
      "Loading file no:-  880\n",
      "Loading file no:-  881\n",
      "Loading file no:-  882\n",
      "Loading file no:-  883\n",
      "Loading file no:-  884\n",
      "Loading file no:-  885\n",
      "Loading file no:-  886\n",
      "Loading file no:-  887\n",
      "Loading file no:-  888\n",
      "Loading file no:-  889\n",
      "Loading file no:-  890\n",
      "Loading file no:-  891\n",
      "Loading file no:-  892\n",
      "Loading file no:-  893\n",
      "Loading file no:-  894\n",
      "Loading file no:-  895\n",
      "Loading file no:-  896\n",
      "Loading file no:-  897\n",
      "Loading file no:-  898\n",
      "Loading file no:-  899\n",
      "Loading file no:-  900\n",
      "Loading file no:-  901\n",
      "Loading file no:-  902\n",
      "Loading file no:-  903\n",
      "Loading file no:-  904\n",
      "Loading file no:-  905\n",
      "Loading file no:-  906\n",
      "Loading file no:-  907\n",
      "Loading file no:-  908\n",
      "Loading file no:-  909\n",
      "Loading file no:-  910\n",
      "Loading file no:-  911\n",
      "Loading file no:-  912\n",
      "Loading file no:-  913\n",
      "Loading file no:-  914\n",
      "Loading file no:-  915\n",
      "Loading file no:-  916\n",
      "Loading file no:-  917\n",
      "Loading file no:-  918\n",
      "Loading file no:-  919\n",
      "Loading file no:-  920\n",
      "Loading file no:-  921\n",
      "Loading file no:-  922\n",
      "Loading file no:-  923\n",
      "Loading file no:-  924\n",
      "Loading file no:-  925\n",
      "Loading file no:-  926\n",
      "Loading file no:-  927\n",
      "Loading file no:-  928\n",
      "Loading file no:-  929\n",
      "Loading file no:-  930\n",
      "Loading file no:-  931\n",
      "Loading file no:-  932\n",
      "Loading file no:-  933\n",
      "Loading file no:-  934\n",
      "Loading file no:-  935\n",
      "Loading file no:-  936\n",
      "Loading file no:-  937\n",
      "Loading file no:-  938\n",
      "Loading file no:-  939\n",
      "Loading file no:-  940\n",
      "Loading file no:-  941\n",
      "Loading file no:-  942\n",
      "Loading file no:-  943\n",
      "Loading file no:-  944\n",
      "Loading file no:-  945\n",
      "Loading file no:-  946\n",
      "Loading file no:-  947\n",
      "Loading file no:-  948\n",
      "Loading file no:-  949\n",
      "Loading file no:-  950\n",
      "Loading file no:-  951\n",
      "Loading file no:-  952\n",
      "Loading file no:-  953\n",
      "Loading file no:-  954\n",
      "Loading file no:-  955\n",
      "Loading file no:-  956\n",
      "Loading file no:-  957\n",
      "Loading file no:-  958\n",
      "Loading file no:-  959\n",
      "Loading file no:-  960\n",
      "Loading file no:-  961\n",
      "Loading file no:-  962\n",
      "Loading file no:-  963\n",
      "Loading file no:-  964\n",
      "Loading file no:-  965\n",
      "Loading file no:-  966\n",
      "Loading file no:-  967\n",
      "Loading file no:-  968\n",
      "Loading file no:-  969\n",
      "Loading file no:-  970\n",
      "Loading file no:-  971\n",
      "Loading file no:-  972\n",
      "Loading file no:-  973\n",
      "Loading file no:-  974\n",
      "Loading file no:-  975\n",
      "Loading file no:-  976\n",
      "Loading file no:-  977\n",
      "Loading file no:-  978\n",
      "Loading file no:-  979\n",
      "Loading file no:-  980\n",
      "Loading file no:-  981\n",
      "Loading file no:-  982\n",
      "Loading file no:-  983\n",
      "Loading file no:-  984\n",
      "Loading file no:-  985\n",
      "Loading file no:-  986\n",
      "Loading file no:-  987\n",
      "Loading file no:-  988\n",
      "Loading file no:-  989\n",
      "Loading file no:-  990\n",
      "Loading file no:-  991\n",
      "Loading file no:-  992\n",
      "Loading file no:-  993\n",
      "Loading file no:-  994\n",
      "Loading file no:-  995\n",
      "Loading file no:-  996\n",
      "Loading file no:-  997\n",
      "Loading file no:-  998\n",
      "Loading file no:-  999\n",
      "Loading file no:-  1000\n",
      "Loading file no:-  1001\n",
      "Loading file no:-  1002\n",
      "Loading file no:-  1003\n",
      "Loading file no:-  1004\n",
      "Loading file no:-  1005\n",
      "Loading file no:-  1006\n",
      "Loading file no:-  1007\n",
      "Loading file no:-  1008\n",
      "Loading file no:-  1009\n",
      "Loading file no:-  1010\n",
      "Loading file no:-  1011\n",
      "Loading file no:-  1012\n",
      "Loading file no:-  1013\n",
      "Loading file no:-  1014\n",
      "Loading file no:-  1015\n",
      "Loading file no:-  1016\n",
      "Loading file no:-  1017\n",
      "Loading file no:-  1018\n",
      "Loading file no:-  1019\n",
      "Loading file no:-  1020\n",
      "Loading file no:-  1021\n",
      "Loading file no:-  1022\n",
      "Loading file no:-  1023\n",
      "Loading file no:-  1024\n",
      "Loading file no:-  1025\n",
      "Loading file no:-  1026\n",
      "Loading file no:-  1027\n",
      "Loading file no:-  1028\n",
      "Loading file no:-  1029\n",
      "Loading file no:-  1030\n",
      "Loading file no:-  1031\n",
      "Loading file no:-  1032\n",
      "Loading file no:-  1033\n",
      "Loading file no:-  1034\n",
      "Loading file no:-  1035\n",
      "Loading file no:-  1036\n",
      "Loading file no:-  1037\n",
      "Loading file no:-  1038\n",
      "Loading file no:-  1039\n",
      "Loading file no:-  1040\n",
      "Loading file no:-  1041\n",
      "Loading file no:-  1042\n",
      "Loading file no:-  1043\n",
      "Loading file no:-  1044\n",
      "Loading file no:-  1045\n",
      "Loading file no:-  1046\n",
      "Loading file no:-  1047\n",
      "Loading file no:-  1048\n",
      "Loading file no:-  1049\n",
      "Loading file no:-  1050\n",
      "Loading file no:-  1051\n",
      "Loading file no:-  1052\n",
      "Loading file no:-  1053\n",
      "Loading file no:-  1054\n",
      "Loading file no:-  1055\n",
      "Loading file no:-  1056\n",
      "Loading file no:-  1057\n",
      "Loading file no:-  1058\n",
      "Loading file no:-  1059\n",
      "Loading file no:-  1060\n",
      "Loading file no:-  1061\n",
      "Loading file no:-  1062\n",
      "Loading file no:-  1063\n",
      "Loading file no:-  1064\n",
      "Loading file no:-  1065\n",
      "Loading file no:-  1066\n",
      "Loading file no:-  1067\n",
      "Loading file no:-  1068\n",
      "Loading file no:-  1069\n",
      "Loading file no:-  1070\n",
      "Loading file no:-  1071\n",
      "Loading file no:-  1072\n",
      "Loading file no:-  1073\n",
      "Loading file no:-  1074\n",
      "Loading file no:-  1075\n",
      "Loading file no:-  1076\n",
      "Loading file no:-  1077\n",
      "Loading file no:-  1078\n",
      "Loading file no:-  1079\n",
      "Loading file no:-  1080\n",
      "Loading file no:-  1081\n",
      "Loading file no:-  1082\n",
      "Loading file no:-  1083\n",
      "Loading file no:-  1084\n",
      "Loading file no:-  1085\n",
      "Loading file no:-  1086\n",
      "Loading file no:-  1087\n",
      "Loading file no:-  1088\n",
      "Loading file no:-  1089\n",
      "Loading file no:-  1090\n",
      "Loading file no:-  1091\n",
      "Loading file no:-  1092\n",
      "Loading file no:-  1093\n",
      "Loading file no:-  1094\n",
      "Loading file no:-  1095\n",
      "Loading file no:-  1096\n",
      "Loading file no:-  1097\n",
      "Loading file no:-  1098\n",
      "Loading file no:-  1099\n",
      "Loading file no:-  1100\n",
      "Loading file no:-  1101\n",
      "Loading file no:-  1102\n",
      "Loading file no:-  1103\n",
      "Loading file no:-  1104\n",
      "Loading file no:-  1105\n",
      "Loading file no:-  1106\n",
      "Loading file no:-  1107\n",
      "Loading file no:-  1108\n",
      "Loading file no:-  1109\n",
      "Loading file no:-  1110\n",
      "Loading file no:-  1111\n",
      "Loading file no:-  1112\n",
      "Loading file no:-  1113\n",
      "Loading file no:-  1114\n",
      "Loading file no:-  1115\n",
      "Loading file no:-  1116\n",
      "Loading file no:-  1117\n",
      "Loading file no:-  1118\n",
      "Loading file no:-  1119\n",
      "Loading file no:-  1120\n",
      "Loading file no:-  1121\n",
      "Loading file no:-  1122\n",
      "Loading file no:-  1123\n",
      "Loading file no:-  1124\n",
      "Loading file no:-  1125\n",
      "Loading file no:-  1126\n",
      "Loading file no:-  1127\n",
      "Loading file no:-  1128\n",
      "Loading file no:-  1129\n",
      "Loading file no:-  1130\n",
      "Loading file no:-  1131\n",
      "Loading file no:-  1132\n",
      "Loading file no:-  1133\n",
      "Loading file no:-  1134\n",
      "Loading file no:-  1135\n",
      "Loading file no:-  1136\n",
      "Loading file no:-  1137\n",
      "Loading file no:-  1138\n",
      "Loading file no:-  1139\n",
      "Loading file no:-  1140\n",
      "Loading file no:-  1141\n",
      "Loading file no:-  1142\n",
      "Loading file no:-  1143\n",
      "Loading file no:-  1144\n",
      "Loading file no:-  1145\n",
      "Loading file no:-  1146\n",
      "Loading file no:-  1147\n",
      "Loading file no:-  1148\n",
      "Loading file no:-  1149\n",
      "Loading file no:-  1150\n",
      "Loading file no:-  1151\n",
      "Loading file no:-  1152\n",
      "Loading file no:-  1153\n",
      "Loading file no:-  1154\n",
      "Loading file no:-  1155\n",
      "Loading file no:-  1156\n",
      "Loading file no:-  1157\n",
      "Loading file no:-  1158\n",
      "Loading file no:-  1159\n",
      "Loading file no:-  1160\n",
      "Loading file no:-  1161\n",
      "Loading file no:-  1162\n",
      "Loading file no:-  1163\n",
      "Loading file no:-  1164\n",
      "Loading file no:-  1165\n",
      "Loading file no:-  1166\n",
      "Loading file no:-  1167\n",
      "Loading file no:-  1168\n",
      "Loading file no:-  1169\n",
      "Loading file no:-  1170\n",
      "Loading file no:-  1171\n",
      "Loading file no:-  1172\n",
      "Loading file no:-  1173\n",
      "Loading file no:-  1174\n",
      "Loading file no:-  1175\n",
      "Loading file no:-  1176\n",
      "Loading file no:-  1177\n",
      "Loading file no:-  1178\n",
      "Loading file no:-  1179\n",
      "Loading file no:-  1180\n",
      "Loading file no:-  1181\n",
      "Loading file no:-  1182\n",
      "Loading file no:-  1183\n",
      "Loading file no:-  1184\n",
      "Loading file no:-  1185\n",
      "Loading file no:-  1186\n",
      "Loading file no:-  1187\n",
      "Loading file no:-  1188\n",
      "Loading file no:-  1189\n",
      "Loading file no:-  1190\n",
      "Loading file no:-  1191\n",
      "Loading file no:-  1192\n",
      "Loading file no:-  1193\n",
      "Loading file no:-  1194\n",
      "Loading file no:-  1195\n",
      "Loading file no:-  1196\n",
      "Loading file no:-  1197\n",
      "Loading file no:-  1198\n",
      "Loading file no:-  1199\n",
      "Loading file no:-  1200\n",
      "Loading file no:-  1201\n",
      "Loading file no:-  1202\n",
      "Loading file no:-  1203\n",
      "Loading file no:-  1204\n",
      "Loading file no:-  1205\n",
      "Loading file no:-  1206\n",
      "Loading file no:-  1207\n",
      "Loading file no:-  1208\n",
      "Loading file no:-  1209\n",
      "Loading file no:-  1210\n",
      "Loading file no:-  1211\n",
      "Loading file no:-  1212\n",
      "Loading file no:-  1213\n",
      "Loading file no:-  1214\n",
      "Loading file no:-  1215\n",
      "Loading file no:-  1216\n",
      "Loading file no:-  1217\n",
      "Loading file no:-  1218\n",
      "Loading file no:-  1219\n",
      "Loading file no:-  1220\n",
      "Loading file no:-  1221\n",
      "Loading file no:-  1222\n",
      "Loading file no:-  1223\n",
      "Loading file no:-  1224\n",
      "Loading file no:-  1225\n",
      "Loading file no:-  1226\n",
      "Loading file no:-  1227\n",
      "Loading file no:-  1228\n",
      "Loading file no:-  1229\n",
      "Loading file no:-  1230\n",
      "Loading file no:-  1231\n",
      "Loading file no:-  1232\n",
      "Loading file no:-  1233\n",
      "Loading file no:-  1234\n",
      "Loading file no:-  1235\n",
      "Loading file no:-  1236\n",
      "Loading file no:-  1237\n",
      "Loading file no:-  1238\n",
      "Loading file no:-  1239\n",
      "Loading file no:-  1240\n",
      "Loading file no:-  1241\n",
      "Loading file no:-  1242\n",
      "Loading file no:-  1243\n",
      "Loading file no:-  1244\n",
      "Loading file no:-  1245\n",
      "Loading file no:-  1246\n",
      "Loading file no:-  1247\n",
      "Loading file no:-  1248\n",
      "Loading file no:-  1249\n",
      "Loading file no:-  1250\n",
      "Loading file no:-  1251\n",
      "Loading file no:-  1252\n",
      "Loading file no:-  1253\n",
      "Loading file no:-  1254\n",
      "Loading file no:-  1255\n",
      "Loading file no:-  1256\n",
      "Loading file no:-  1257\n",
      "Loading file no:-  1258\n",
      "Loading file no:-  1259\n",
      "Loading file no:-  1260\n",
      "Loading file no:-  1261\n",
      "Loading file no:-  1262\n",
      "Loading file no:-  1263\n",
      "Loading file no:-  1264\n",
      "Loading file no:-  1265\n",
      "Loading file no:-  1266\n",
      "Loading file no:-  1267\n",
      "Loading file no:-  1268\n",
      "Loading file no:-  1269\n",
      "Loading file no:-  1270\n",
      "Loading file no:-  1271\n",
      "Loading file no:-  1272\n",
      "Loading file no:-  1273\n",
      "Loading file no:-  1274\n",
      "Loading file no:-  1275\n",
      "Loading file no:-  1276\n",
      "Loading file no:-  1277\n",
      "Loading file no:-  1278\n",
      "Loading file no:-  1279\n",
      "Loading file no:-  1280\n",
      "Loading file no:-  1281\n",
      "Loading file no:-  1282\n",
      "Loading file no:-  1283\n",
      "Loading file no:-  1284\n",
      "Loading file no:-  1285\n",
      "Loading file no:-  1286\n",
      "Loading file no:-  1287\n",
      "Loading file no:-  1288\n",
      "Loading file no:-  1289\n",
      "Loading file no:-  1290\n",
      "Loading file no:-  1291\n",
      "Loading file no:-  1292\n",
      "Loading file no:-  1293\n",
      "Loading file no:-  1294\n",
      "Loading file no:-  1295\n",
      "Loading file no:-  1296\n",
      "Loading file no:-  1297\n",
      "Loading file no:-  1298\n",
      "Loading file no:-  1299\n",
      "Loading file no:-  1300\n",
      "Loading file no:-  1301\n",
      "Loading file no:-  1302\n",
      "Loading file no:-  1303\n",
      "Loading file no:-  1304\n",
      "Loading file no:-  1305\n",
      "Loading file no:-  1306\n",
      "Loading file no:-  1307\n",
      "Loading file no:-  1308\n",
      "Loading file no:-  1309\n",
      "Loading file no:-  1310\n",
      "Loading file no:-  1311\n",
      "Loading file no:-  1312\n",
      "Loading file no:-  1313\n",
      "Loading file no:-  1314\n",
      "Loading file no:-  1315\n",
      "Loading file no:-  1316\n",
      "Loading file no:-  1317\n",
      "Loading file no:-  1318\n",
      "Loading file no:-  1319\n",
      "Loading file no:-  1320\n",
      "Loading file no:-  1321\n",
      "Loading file no:-  1322\n",
      "Loading file no:-  1323\n",
      "Loading file no:-  1324\n",
      "Loading file no:-  1325\n",
      "Loading file no:-  1326\n",
      "Loading file no:-  1327\n",
      "Loading file no:-  1328\n",
      "Loading file no:-  1329\n",
      "Loading file no:-  1330\n",
      "Loading file no:-  1331\n",
      "Loading file no:-  1332\n",
      "Loading file no:-  1333\n",
      "Loading file no:-  1334\n",
      "Loading file no:-  1335\n",
      "Loading file no:-  1336\n",
      "Loading file no:-  1337\n",
      "Loading file no:-  1338\n",
      "Loading file no:-  1339\n",
      "Loading file no:-  1340\n",
      "Loading file no:-  1341\n",
      "Loading file no:-  1342\n",
      "Loading file no:-  1343\n",
      "Loading file no:-  1344\n",
      "Loading file no:-  1345\n",
      "Loading file no:-  1346\n",
      "Loading file no:-  1347\n",
      "Loading file no:-  1348\n",
      "Loading file no:-  1349\n",
      "Loading file no:-  1350\n",
      "Loading file no:-  1351\n",
      "Loading file no:-  1352\n",
      "Loading file no:-  1353\n",
      "Loading file no:-  1354\n",
      "Loading file no:-  1355\n",
      "Loading file no:-  1356\n",
      "Loading file no:-  1357\n",
      "Loading file no:-  1358\n",
      "Loading file no:-  1359\n",
      "Loading file no:-  1360\n",
      "Loading file no:-  1361\n",
      "Loading file no:-  1362\n",
      "Loading file no:-  1363\n",
      "Loading file no:-  1364\n",
      "Loading file no:-  1365\n",
      "Loading file no:-  1366\n",
      "Loading file no:-  1367\n",
      "Loading file no:-  1368\n",
      "Loading file no:-  1369\n",
      "Loading file no:-  1370\n",
      "Loading file no:-  1371\n",
      "Loading file no:-  1372\n",
      "Loading file no:-  1373\n",
      "Loading file no:-  1374\n",
      "Loading file no:-  1375\n",
      "Loading file no:-  1376\n",
      "Loading file no:-  1377\n",
      "Loading file no:-  1378\n",
      "Loading file no:-  1379\n",
      "Loading file no:-  1380\n",
      "Loading file no:-  1381\n",
      "Loading file no:-  1382\n",
      "Loading file no:-  1383\n",
      "Loading file no:-  1384\n",
      "Loading file no:-  1385\n",
      "Loading file no:-  1386\n",
      "Loading file no:-  1387\n",
      "Loading file no:-  1388\n",
      "Loading file no:-  1389\n",
      "Loading file no:-  1390\n",
      "Loading file no:-  1391\n",
      "Loading file no:-  1392\n",
      "Loading file no:-  1393\n",
      "Loading file no:-  1394\n",
      "Loading file no:-  1395\n",
      "Loading file no:-  1396\n",
      "Loading file no:-  1397\n",
      "Loading file no:-  1398\n",
      "Loading file no:-  1399\n",
      "Loading file no:-  1400\n",
      "Loading file no:-  1401\n",
      "Loading file no:-  1402\n",
      "Loading file no:-  1403\n",
      "Loading file no:-  1404\n",
      "Loading file no:-  1405\n",
      "Loading file no:-  1406\n",
      "Loading file no:-  1407\n",
      "Loading file no:-  1408\n",
      "Loading file no:-  1409\n",
      "Loading file no:-  1410\n",
      "Loading file no:-  1411\n",
      "Loading file no:-  1412\n",
      "Loading file no:-  1413\n",
      "Loading file no:-  1414\n",
      "Loading file no:-  1415\n",
      "Loading file no:-  1416\n",
      "Loading file no:-  1417\n",
      "Loading file no:-  1418\n",
      "Loading file no:-  1419\n",
      "Loading file no:-  1420\n",
      "Loading file no:-  1421\n",
      "Loading file no:-  1422\n",
      "Loading file no:-  1423\n",
      "Loading file no:-  1424\n",
      "Loading file no:-  1425\n",
      "Loading file no:-  1426\n",
      "Loading file no:-  1427\n",
      "Loading file no:-  1428\n",
      "Loading file no:-  1429\n",
      "Loading file no:-  1430\n",
      "Loading file no:-  1431\n",
      "Loading file no:-  1432\n",
      "Loading file no:-  1433\n",
      "Loading file no:-  1434\n",
      "Loading file no:-  1435\n",
      "Loading file no:-  1436\n",
      "Loading file no:-  1437\n",
      "Loading file no:-  1438\n",
      "Loading file no:-  1439\n",
      "Loading file no:-  1440\n",
      "Loading file no:-  1441\n",
      "Loading file no:-  1442\n",
      "Loading file no:-  1443\n",
      "Loading file no:-  1444\n",
      "Loading file no:-  1445\n",
      "Loading file no:-  1446\n",
      "Loading file no:-  1447\n",
      "Loading file no:-  1448\n",
      "Loading file no:-  1449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -  0  Loss -  tensor(0.1202, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  1  Loss -  tensor(0.1192, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  2  Loss -  tensor(0.1187, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  3  Loss -  tensor(0.1183, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  4  Loss -  tensor(0.1175, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  5  Loss -  tensor(0.1167, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  6  Loss -  tensor(0.1158, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  7  Loss -  tensor(0.1146, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  8  Loss -  tensor(0.1138, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  9  Loss -  tensor(0.1129, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  10  Loss -  tensor(0.1122, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  11  Loss -  tensor(0.1117, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  12  Loss -  tensor(0.1098, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  13  Loss -  tensor(0.1090, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  14  Loss -  tensor(0.1081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  15  Loss -  tensor(0.1066, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  16  Loss -  tensor(0.1053, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  17  Loss -  tensor(0.1041, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  18  Loss -  tensor(0.1023, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  19  Loss -  tensor(0.0999, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  20  Loss -  tensor(0.1003, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  21  Loss -  tensor(0.0983, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  22  Loss -  tensor(0.0974, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  23  Loss -  tensor(0.0963, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  24  Loss -  tensor(0.0937, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  25  Loss -  tensor(0.0924, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  26  Loss -  tensor(0.0904, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  27  Loss -  tensor(0.0909, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  28  Loss -  tensor(0.0899, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  29  Loss -  tensor(0.0897, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  30  Loss -  tensor(0.0875, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  31  Loss -  tensor(0.0847, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  32  Loss -  tensor(0.0876, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  33  Loss -  tensor(0.0827, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  34  Loss -  tensor(0.0865, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  35  Loss -  tensor(0.0812, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  36  Loss -  tensor(0.0810, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  37  Loss -  tensor(0.0785, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  38  Loss -  tensor(0.0791, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  39  Loss -  tensor(0.0760, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  40  Loss -  tensor(0.0763, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  41  Loss -  tensor(0.0755, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  42  Loss -  tensor(0.0797, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  43  Loss -  tensor(0.0741, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  44  Loss -  tensor(0.0757, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  45  Loss -  tensor(0.0728, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  46  Loss -  tensor(0.0727, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  47  Loss -  tensor(0.0756, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  48  Loss -  tensor(0.0737, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  49  Loss -  tensor(0.0750, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  50  Loss -  tensor(0.0719, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  51  Loss -  tensor(0.0730, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  52  Loss -  tensor(0.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  53  Loss -  tensor(0.0700, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  54  Loss -  tensor(0.0720, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  55  Loss -  tensor(0.0679, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  56  Loss -  tensor(0.0719, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  57  Loss -  tensor(0.0705, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  58  Loss -  tensor(0.0701, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  59  Loss -  tensor(0.0712, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  60  Loss -  tensor(0.0711, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  61  Loss -  tensor(0.0690, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  62  Loss -  tensor(0.0714, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  63  Loss -  tensor(0.0706, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  64  Loss -  tensor(0.0679, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  65  Loss -  tensor(0.0691, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  66  Loss -  tensor(0.0685, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  67  Loss -  tensor(0.0685, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  68  Loss -  tensor(0.0706, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  69  Loss -  tensor(0.0677, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  70  Loss -  tensor(0.0681, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  71  Loss -  tensor(0.0683, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  72  Loss -  tensor(0.0695, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  73  Loss -  tensor(0.0676, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  74  Loss -  tensor(0.0676, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  75  Loss -  tensor(0.0660, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  76  Loss -  tensor(0.0700, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  77  Loss -  tensor(0.0673, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  78  Loss -  tensor(0.0675, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  79  Loss -  tensor(0.0660, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  80  Loss -  tensor(0.0692, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  81  Loss -  tensor(0.0658, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  82  Loss -  tensor(0.0710, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  83  Loss -  tensor(0.0662, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  84  Loss -  tensor(0.0681, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  85  Loss -  tensor(0.0655, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  86  Loss -  tensor(0.0656, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  87  Loss -  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  88  Loss -  tensor(0.0697, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  89  Loss -  tensor(0.0677, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  90  Loss -  tensor(0.0662, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  91  Loss -  tensor(0.0670, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  92  Loss -  tensor(0.0653, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  93  Loss -  tensor(0.0646, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  94  Loss -  tensor(0.0667, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  95  Loss -  tensor(0.0656, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  96  Loss -  tensor(0.0680, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  97  Loss -  tensor(0.0655, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  98  Loss -  tensor(0.0676, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  99  Loss -  tensor(0.0655, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  100  Loss -  tensor(0.0651, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  101  Loss -  tensor(0.0672, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  102  Loss -  tensor(0.0652, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  103  Loss -  tensor(0.0644, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  104  Loss -  tensor(0.0662, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  105  Loss -  tensor(0.0660, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  106  Loss -  tensor(0.0681, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  107  Loss -  tensor(0.0628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  108  Loss -  tensor(0.0664, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  109  Loss -  tensor(0.0675, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  110  Loss -  tensor(0.0641, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  111  Loss -  tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  112  Loss -  tensor(0.0665, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  113  Loss -  tensor(0.0686, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  114  Loss -  tensor(0.0673, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  115  Loss -  tensor(0.0646, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  116  Loss -  tensor(0.0658, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  117  Loss -  tensor(0.0673, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  118  Loss -  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  119  Loss -  tensor(0.0633, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  120  Loss -  tensor(0.0701, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  121  Loss -  tensor(0.0647, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  122  Loss -  tensor(0.0653, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  123  Loss -  tensor(0.0634, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  124  Loss -  tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  125  Loss -  tensor(0.0634, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  126  Loss -  tensor(0.0671, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  127  Loss -  tensor(0.0661, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  128  Loss -  tensor(0.0648, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  129  Loss -  tensor(0.0661, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  130  Loss -  tensor(0.0659, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  131  Loss -  tensor(0.0633, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  132  Loss -  tensor(0.0671, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  133  Loss -  tensor(0.0674, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  134  Loss -  tensor(0.0679, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  135  Loss -  tensor(0.0667, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  136  Loss -  tensor(0.0679, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  137  Loss -  tensor(0.0664, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  138  Loss -  tensor(0.0650, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  139  Loss -  tensor(0.0669, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  140  Loss -  tensor(0.0635, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  141  Loss -  tensor(0.0665, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  142  Loss -  tensor(0.0661, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  143  Loss -  tensor(0.0669, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  144  Loss -  tensor(0.0644, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  145  Loss -  tensor(0.0657, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  146  Loss -  tensor(0.0668, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  147  Loss -  tensor(0.0646, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  148  Loss -  tensor(0.0675, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  149  Loss -  tensor(0.0648, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  150  Loss -  tensor(0.0661, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  151  Loss -  tensor(0.0628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  152  Loss -  tensor(0.0658, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  153  Loss -  tensor(0.0661, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  154  Loss -  tensor(0.0660, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  155  Loss -  tensor(0.0643, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  156  Loss -  tensor(0.0678, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  157  Loss -  tensor(0.0641, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  158  Loss -  tensor(0.0632, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  159  Loss -  tensor(0.0662, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  160  Loss -  tensor(0.0652, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  161  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  162  Loss -  tensor(0.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  163  Loss -  tensor(0.0655, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  164  Loss -  tensor(0.0675, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  165  Loss -  tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  166  Loss -  tensor(0.0643, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  167  Loss -  tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  168  Loss -  tensor(0.0633, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  169  Loss -  tensor(0.0654, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  170  Loss -  tensor(0.0648, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  171  Loss -  tensor(0.0643, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  172  Loss -  tensor(0.0623, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  173  Loss -  tensor(0.0661, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  174  Loss -  tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  175  Loss -  tensor(0.0660, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  176  Loss -  tensor(0.0648, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  177  Loss -  tensor(0.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  178  Loss -  tensor(0.0664, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  179  Loss -  tensor(0.0655, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  180  Loss -  tensor(0.0647, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  181  Loss -  tensor(0.0645, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  182  Loss -  tensor(0.0647, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  183  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  184  Loss -  tensor(0.0644, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  185  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  186  Loss -  tensor(0.0637, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  187  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  188  Loss -  tensor(0.0651, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  189  Loss -  tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  190  Loss -  tensor(0.0624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  191  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  192  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  193  Loss -  tensor(0.0664, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  194  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  195  Loss -  tensor(0.0632, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  196  Loss -  tensor(0.0622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  197  Loss -  tensor(0.0646, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  198  Loss -  tensor(0.0651, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  199  Loss -  tensor(0.0659, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  200  Loss -  tensor(0.0632, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  201  Loss -  tensor(0.0623, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  202  Loss -  tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  203  Loss -  tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  204  Loss -  tensor(0.0647, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  205  Loss -  tensor(0.0654, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  206  Loss -  tensor(0.0653, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  207  Loss -  tensor(0.0638, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  208  Loss -  tensor(0.0645, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  209  Loss -  tensor(0.0643, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  210  Loss -  tensor(0.0635, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  211  Loss -  tensor(0.0628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  212  Loss -  tensor(0.0651, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  213  Loss -  tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  214  Loss -  tensor(0.0656, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  215  Loss -  tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  216  Loss -  tensor(0.0643, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  217  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  218  Loss -  tensor(0.0645, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  219  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  220  Loss -  tensor(0.0642, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  221  Loss -  tensor(0.0654, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  222  Loss -  tensor(0.0616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  223  Loss -  tensor(0.0628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  224  Loss -  tensor(0.0685, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  225  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  226  Loss -  tensor(0.0659, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  227  Loss -  tensor(0.0644, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  228  Loss -  tensor(0.0613, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  229  Loss -  tensor(0.0627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  230  Loss -  tensor(0.0647, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  231  Loss -  tensor(0.0647, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  232  Loss -  tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  233  Loss -  tensor(0.0611, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  234  Loss -  tensor(0.0672, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  235  Loss -  tensor(0.0627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  236  Loss -  tensor(0.0661, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  237  Loss -  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  238  Loss -  tensor(0.0647, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  239  Loss -  tensor(0.0631, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  240  Loss -  tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  241  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  242  Loss -  tensor(0.0619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  243  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  244  Loss -  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  245  Loss -  tensor(0.0616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  246  Loss -  tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  247  Loss -  tensor(0.0669, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  248  Loss -  tensor(0.0654, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  249  Loss -  tensor(0.0608, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  250  Loss -  tensor(0.0654, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  251  Loss -  tensor(0.0602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  252  Loss -  tensor(0.0646, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  253  Loss -  tensor(0.0640, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  254  Loss -  tensor(0.0620, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  255  Loss -  tensor(0.0630, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  256  Loss -  tensor(0.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  257  Loss -  tensor(0.0649, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  258  Loss -  tensor(0.0620, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  259  Loss -  tensor(0.0641, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  260  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  261  Loss -  tensor(0.0649, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  262  Loss -  tensor(0.0660, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  263  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  264  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  265  Loss -  tensor(0.0628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  266  Loss -  tensor(0.0649, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  267  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  268  Loss -  tensor(0.0581, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  269  Loss -  tensor(0.0657, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  270  Loss -  tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  271  Loss -  tensor(0.0646, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  272  Loss -  tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  273  Loss -  tensor(0.0661, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  274  Loss -  tensor(0.0627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  275  Loss -  tensor(0.0624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  276  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  277  Loss -  tensor(0.0623, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  278  Loss -  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  279  Loss -  tensor(0.0642, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  280  Loss -  tensor(0.0638, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  281  Loss -  tensor(0.0637, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  282  Loss -  tensor(0.0692, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  283  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  284  Loss -  tensor(0.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  285  Loss -  tensor(0.0639, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  286  Loss -  tensor(0.0649, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  287  Loss -  tensor(0.0647, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  288  Loss -  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  289  Loss -  tensor(0.0620, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  290  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  291  Loss -  tensor(0.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  292  Loss -  tensor(0.0631, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  293  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  294  Loss -  tensor(0.0598, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  295  Loss -  tensor(0.0627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  296  Loss -  tensor(0.0623, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  297  Loss -  tensor(0.0604, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  298  Loss -  tensor(0.0668, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  299  Loss -  tensor(0.0602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  300  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  301  Loss -  tensor(0.0638, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  302  Loss -  tensor(0.0657, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  303  Loss -  tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  304  Loss -  tensor(0.0641, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  305  Loss -  tensor(0.0648, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  306  Loss -  tensor(0.0622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  307  Loss -  tensor(0.0646, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  308  Loss -  tensor(0.0620, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  309  Loss -  tensor(0.0613, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  310  Loss -  tensor(0.0650, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  311  Loss -  tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  312  Loss -  tensor(0.0670, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  313  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  314  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  315  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  316  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  317  Loss -  tensor(0.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  318  Loss -  tensor(0.0637, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  319  Loss -  tensor(0.0608, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  320  Loss -  tensor(0.0642, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  321  Loss -  tensor(0.0657, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  322  Loss -  tensor(0.0663, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  323  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  324  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  325  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  326  Loss -  tensor(0.0642, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  327  Loss -  tensor(0.0627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  328  Loss -  tensor(0.0632, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  329  Loss -  tensor(0.0620, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  330  Loss -  tensor(0.0616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  331  Loss -  tensor(0.0616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  332  Loss -  tensor(0.0618, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  333  Loss -  tensor(0.0633, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  334  Loss -  tensor(0.0638, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  335  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  336  Loss -  tensor(0.0656, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  337  Loss -  tensor(0.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  338  Loss -  tensor(0.0638, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  339  Loss -  tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  340  Loss -  tensor(0.0649, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  341  Loss -  tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  342  Loss -  tensor(0.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  343  Loss -  tensor(0.0619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  344  Loss -  tensor(0.0620, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  345  Loss -  tensor(0.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  346  Loss -  tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  347  Loss -  tensor(0.0652, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  348  Loss -  tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  349  Loss -  tensor(0.0611, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  350  Loss -  tensor(0.0628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  351  Loss -  tensor(0.0655, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  352  Loss -  tensor(0.0640, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  353  Loss -  tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  354  Loss -  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  355  Loss -  tensor(0.0598, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  356  Loss -  tensor(0.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  357  Loss -  tensor(0.0640, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  358  Loss -  tensor(0.0643, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  359  Loss -  tensor(0.0620, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  360  Loss -  tensor(0.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  361  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  362  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  363  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  364  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  365  Loss -  tensor(0.0661, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  366  Loss -  tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  367  Loss -  tensor(0.0640, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  368  Loss -  tensor(0.0676, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  369  Loss -  tensor(0.0643, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  370  Loss -  tensor(0.0653, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  371  Loss -  tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  372  Loss -  tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  373  Loss -  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  374  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  375  Loss -  tensor(0.0656, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  376  Loss -  tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  377  Loss -  tensor(0.0613, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  378  Loss -  tensor(0.1057, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -  0  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  1  Loss -  tensor(0.0628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  2  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  3  Loss -  tensor(0.0627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  4  Loss -  tensor(0.0618, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  5  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  6  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  7  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  8  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  9  Loss -  tensor(0.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  10  Loss -  tensor(0.0666, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  11  Loss -  tensor(0.0616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  12  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  13  Loss -  tensor(0.0616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  14  Loss -  tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  15  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  16  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  17  Loss -  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  18  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  19  Loss -  tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  20  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  21  Loss -  tensor(0.0630, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  22  Loss -  tensor(0.0644, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  23  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  24  Loss -  tensor(0.0616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  25  Loss -  tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  26  Loss -  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  27  Loss -  tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  28  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  29  Loss -  tensor(0.0618, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  30  Loss -  tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  31  Loss -  tensor(0.0611, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  32  Loss -  tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  33  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  34  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  35  Loss -  tensor(0.0602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  36  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  37  Loss -  tensor(0.0635, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  38  Loss -  tensor(0.0624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  39  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  40  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  41  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  42  Loss -  tensor(0.0632, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  43  Loss -  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  44  Loss -  tensor(0.0624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  45  Loss -  tensor(0.0655, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  46  Loss -  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  47  Loss -  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  48  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  49  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  50  Loss -  tensor(0.0618, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  51  Loss -  tensor(0.0624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  52  Loss -  tensor(0.0645, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  53  Loss -  tensor(0.0624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  54  Loss -  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  55  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  56  Loss -  tensor(0.0627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  57  Loss -  tensor(0.0640, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  58  Loss -  tensor(0.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  59  Loss -  tensor(0.0623, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  60  Loss -  tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  61  Loss -  tensor(0.0619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  62  Loss -  tensor(0.0637, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  63  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  64  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  65  Loss -  tensor(0.0632, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  66  Loss -  tensor(0.0623, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  67  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  68  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  69  Loss -  tensor(0.0644, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  70  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  71  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  72  Loss -  tensor(0.0608, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  73  Loss -  tensor(0.0630, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  74  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  75  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  76  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  77  Loss -  tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  78  Loss -  tensor(0.0627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  79  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  80  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  81  Loss -  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  82  Loss -  tensor(0.0632, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  83  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  84  Loss -  tensor(0.0648, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  85  Loss -  tensor(0.0662, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  86  Loss -  tensor(0.0648, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  87  Loss -  tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  88  Loss -  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  89  Loss -  tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  90  Loss -  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  91  Loss -  tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  92  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  93  Loss -  tensor(0.0662, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  94  Loss -  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  95  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  96  Loss -  tensor(0.0627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  97  Loss -  tensor(0.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  98  Loss -  tensor(0.0637, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  99  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  100  Loss -  tensor(0.0620, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  101  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  102  Loss -  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  103  Loss -  tensor(0.0611, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  104  Loss -  tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  105  Loss -  tensor(0.0635, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  106  Loss -  tensor(0.0630, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  107  Loss -  tensor(0.0623, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  108  Loss -  tensor(0.0616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  109  Loss -  tensor(0.0622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  110  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  111  Loss -  tensor(0.0620, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  112  Loss -  tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  113  Loss -  tensor(0.0608, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  114  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  115  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  116  Loss -  tensor(0.0624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  117  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  118  Loss -  tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  119  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  120  Loss -  tensor(0.0556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  121  Loss -  tensor(0.0635, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  122  Loss -  tensor(0.0630, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  123  Loss -  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  124  Loss -  tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  125  Loss -  tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  126  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  127  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  128  Loss -  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  129  Loss -  tensor(0.0682, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  130  Loss -  tensor(0.0632, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  131  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  132  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  133  Loss -  tensor(0.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  134  Loss -  tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  135  Loss -  tensor(0.0623, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  136  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  137  Loss -  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  138  Loss -  tensor(0.0613, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  139  Loss -  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  140  Loss -  tensor(0.0632, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  141  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  142  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  143  Loss -  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  144  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  145  Loss -  tensor(0.0644, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  146  Loss -  tensor(0.0604, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  147  Loss -  tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  148  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  149  Loss -  tensor(0.0622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  150  Loss -  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  151  Loss -  tensor(0.0631, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  152  Loss -  tensor(0.0637, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  153  Loss -  tensor(0.0619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  154  Loss -  tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  155  Loss -  tensor(0.0642, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  156  Loss -  tensor(0.0639, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  157  Loss -  tensor(0.0658, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  158  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  159  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  160  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  161  Loss -  tensor(0.0611, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  162  Loss -  tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  163  Loss -  tensor(0.0598, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  164  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  165  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  166  Loss -  tensor(0.0640, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  167  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  168  Loss -  tensor(0.0602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  169  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  170  Loss -  tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  171  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  172  Loss -  tensor(0.0556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  173  Loss -  tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  174  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  175  Loss -  tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  176  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  177  Loss -  tensor(0.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  178  Loss -  tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  179  Loss -  tensor(0.0611, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  180  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  181  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  182  Loss -  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  183  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  184  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  185  Loss -  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  186  Loss -  tensor(0.0661, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  187  Loss -  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  188  Loss -  tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  189  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  190  Loss -  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  191  Loss -  tensor(0.0635, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  192  Loss -  tensor(0.0637, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  193  Loss -  tensor(0.0643, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  194  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  195  Loss -  tensor(0.0642, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  196  Loss -  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  197  Loss -  tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  198  Loss -  tensor(0.0598, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  199  Loss -  tensor(0.0611, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  200  Loss -  tensor(0.0563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  201  Loss -  tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  202  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  203  Loss -  tensor(0.0616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  204  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  205  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  206  Loss -  tensor(0.0622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  207  Loss -  tensor(0.0644, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  208  Loss -  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  209  Loss -  tensor(0.0624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  210  Loss -  tensor(0.0628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  211  Loss -  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  212  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  213  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  214  Loss -  tensor(0.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  215  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  216  Loss -  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  217  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  218  Loss -  tensor(0.0619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  219  Loss -  tensor(0.0633, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  220  Loss -  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  221  Loss -  tensor(0.0633, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  222  Loss -  tensor(0.0602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  223  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  224  Loss -  tensor(0.0639, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  225  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  226  Loss -  tensor(0.0631, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  227  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  228  Loss -  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  229  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  230  Loss -  tensor(0.0661, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  231  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  232  Loss -  tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  233  Loss -  tensor(0.0619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  234  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  235  Loss -  tensor(0.0632, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  236  Loss -  tensor(0.0576, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  237  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  238  Loss -  tensor(0.0637, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  239  Loss -  tensor(0.0622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  240  Loss -  tensor(0.0576, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  241  Loss -  tensor(0.0611, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  242  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  243  Loss -  tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  244  Loss -  tensor(0.0647, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  245  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  246  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  247  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  248  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  249  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  250  Loss -  tensor(0.0613, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  251  Loss -  tensor(0.0618, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  252  Loss -  tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  253  Loss -  tensor(0.0608, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  254  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  255  Loss -  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  256  Loss -  tensor(0.0619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  257  Loss -  tensor(0.0623, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  258  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  259  Loss -  tensor(0.0647, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  260  Loss -  tensor(0.0581, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  261  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  262  Loss -  tensor(0.0616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  263  Loss -  tensor(0.0646, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  264  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  265  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  266  Loss -  tensor(0.0644, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  267  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  268  Loss -  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  269  Loss -  tensor(0.0646, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  270  Loss -  tensor(0.0613, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  271  Loss -  tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  272  Loss -  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  273  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  274  Loss -  tensor(0.0620, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  275  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  276  Loss -  tensor(0.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  277  Loss -  tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  278  Loss -  tensor(0.0640, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  279  Loss -  tensor(0.0608, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  280  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  281  Loss -  tensor(0.0642, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  282  Loss -  tensor(0.0628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  283  Loss -  tensor(0.0624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  284  Loss -  tensor(0.0634, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  285  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  286  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  287  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  288  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  289  Loss -  tensor(0.0644, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  290  Loss -  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  291  Loss -  tensor(0.0648, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  292  Loss -  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  293  Loss -  tensor(0.0652, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  294  Loss -  tensor(0.0623, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  295  Loss -  tensor(0.0632, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  296  Loss -  tensor(0.0648, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  297  Loss -  tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  298  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  299  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  300  Loss -  tensor(0.0649, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  301  Loss -  tensor(0.0563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  302  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  303  Loss -  tensor(0.0616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  304  Loss -  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  305  Loss -  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  306  Loss -  tensor(0.0630, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  307  Loss -  tensor(0.0635, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  308  Loss -  tensor(0.0604, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  309  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  310  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  311  Loss -  tensor(0.0611, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  312  Loss -  tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  313  Loss -  tensor(0.0651, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  314  Loss -  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  315  Loss -  tensor(0.0627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  316  Loss -  tensor(0.0633, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  317  Loss -  tensor(0.0620, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  318  Loss -  tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  319  Loss -  tensor(0.0598, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  320  Loss -  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  321  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  322  Loss -  tensor(0.0602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  323  Loss -  tensor(0.0639, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  324  Loss -  tensor(0.0613, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  325  Loss -  tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  326  Loss -  tensor(0.0604, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  327  Loss -  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  328  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  329  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  330  Loss -  tensor(0.0604, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  331  Loss -  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  332  Loss -  tensor(0.0608, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  333  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  334  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  335  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  336  Loss -  tensor(0.0598, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  337  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  338  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  339  Loss -  tensor(0.0662, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  340  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  341  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  342  Loss -  tensor(0.0638, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  343  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  344  Loss -  tensor(0.0631, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  345  Loss -  tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  346  Loss -  tensor(0.0619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  347  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  348  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  349  Loss -  tensor(0.0598, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  350  Loss -  tensor(0.0622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  351  Loss -  tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  352  Loss -  tensor(0.0638, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  353  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  354  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  355  Loss -  tensor(0.0581, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  356  Loss -  tensor(0.0650, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  357  Loss -  tensor(0.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  358  Loss -  tensor(0.0623, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  359  Loss -  tensor(0.0619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  360  Loss -  tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  361  Loss -  tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  362  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  363  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  364  Loss -  tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  365  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  366  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  367  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  368  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  369  Loss -  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  370  Loss -  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  371  Loss -  tensor(0.0586, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  372  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  373  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  374  Loss -  tensor(0.0627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  375  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  376  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  377  Loss -  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  378  Loss -  tensor(0.1015, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -  0  Loss -  tensor(0.0622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  1  Loss -  tensor(0.0618, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  2  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  3  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  4  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  5  Loss -  tensor(0.0633, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  6  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  7  Loss -  tensor(0.0630, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  8  Loss -  tensor(0.0620, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  9  Loss -  tensor(0.0639, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  10  Loss -  tensor(0.0633, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  11  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  12  Loss -  tensor(0.0613, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  13  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  14  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  15  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  16  Loss -  tensor(0.0641, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  17  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  18  Loss -  tensor(0.0645, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  19  Loss -  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  20  Loss -  tensor(0.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  21  Loss -  tensor(0.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  22  Loss -  tensor(0.0586, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  23  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  24  Loss -  tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  25  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  26  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  27  Loss -  tensor(0.0637, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  28  Loss -  tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  29  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  30  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  31  Loss -  tensor(0.0608, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  32  Loss -  tensor(0.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  33  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  34  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  35  Loss -  tensor(0.0602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  36  Loss -  tensor(0.0577, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  37  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  38  Loss -  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  39  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  40  Loss -  tensor(0.0620, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  41  Loss -  tensor(0.0627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  42  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  43  Loss -  tensor(0.0604, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  44  Loss -  tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  45  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  46  Loss -  tensor(0.0613, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  47  Loss -  tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  48  Loss -  tensor(0.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  49  Loss -  tensor(0.0622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  50  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  51  Loss -  tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  52  Loss -  tensor(0.0598, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  53  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  54  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  55  Loss -  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  56  Loss -  tensor(0.0561, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  57  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  58  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  59  Loss -  tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  60  Loss -  tensor(0.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  61  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  62  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  63  Loss -  tensor(0.0619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  64  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  65  Loss -  tensor(0.0646, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  66  Loss -  tensor(0.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  67  Loss -  tensor(0.0618, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  68  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  69  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  70  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  71  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  72  Loss -  tensor(0.0643, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  73  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  74  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  75  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  76  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  77  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  78  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  79  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  80  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  81  Loss -  tensor(0.0630, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  82  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  83  Loss -  tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  84  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  85  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  86  Loss -  tensor(0.0602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  87  Loss -  tensor(0.0630, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  88  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  89  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  90  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  91  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  92  Loss -  tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  93  Loss -  tensor(0.0608, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  94  Loss -  tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  95  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  96  Loss -  tensor(0.0630, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  97  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  98  Loss -  tensor(0.0628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  99  Loss -  tensor(0.0613, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  100  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  101  Loss -  tensor(0.0622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  102  Loss -  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  103  Loss -  tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  104  Loss -  tensor(0.0623, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  105  Loss -  tensor(0.0630, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  106  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  107  Loss -  tensor(0.0622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  108  Loss -  tensor(0.0659, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  109  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  110  Loss -  tensor(0.0613, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  111  Loss -  tensor(0.0611, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  112  Loss -  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  113  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  114  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  115  Loss -  tensor(0.0604, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  116  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  117  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  118  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  119  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  120  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  121  Loss -  tensor(0.0576, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  122  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  123  Loss -  tensor(0.0581, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  124  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  125  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  126  Loss -  tensor(0.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  127  Loss -  tensor(0.0611, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  128  Loss -  tensor(0.0598, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  129  Loss -  tensor(0.0577, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  130  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  131  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  132  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  133  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  134  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  135  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  136  Loss -  tensor(0.0637, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  137  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  138  Loss -  tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  139  Loss -  tensor(0.0628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  140  Loss -  tensor(0.0640, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  141  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  142  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  143  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  144  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  145  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  146  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  147  Loss -  tensor(0.0653, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  148  Loss -  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  149  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  150  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  151  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  152  Loss -  tensor(0.0602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  153  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  154  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  155  Loss -  tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  156  Loss -  tensor(0.0604, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  157  Loss -  tensor(0.0646, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  158  Loss -  tensor(0.0619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  159  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  160  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  161  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  162  Loss -  tensor(0.0602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  163  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  164  Loss -  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  165  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  166  Loss -  tensor(0.0577, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  167  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  168  Loss -  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  169  Loss -  tensor(0.0619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  170  Loss -  tensor(0.0586, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  171  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  172  Loss -  tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  173  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  174  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  175  Loss -  tensor(0.0627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  176  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  177  Loss -  tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  178  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  179  Loss -  tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  180  Loss -  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  181  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  182  Loss -  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  183  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  184  Loss -  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  185  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  186  Loss -  tensor(0.0563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  187  Loss -  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  188  Loss -  tensor(0.0575, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  189  Loss -  tensor(0.0611, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  190  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  191  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  192  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  193  Loss -  tensor(0.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  194  Loss -  tensor(0.0616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  195  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  196  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  197  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  198  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  199  Loss -  tensor(0.0628, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  200  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  201  Loss -  tensor(0.0631, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  202  Loss -  tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  203  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  204  Loss -  tensor(0.0586, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  205  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  206  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  207  Loss -  tensor(0.0632, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  208  Loss -  tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  209  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  210  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  211  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  212  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  213  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  214  Loss -  tensor(0.0604, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  215  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  216  Loss -  tensor(0.0586, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  217  Loss -  tensor(0.0608, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  218  Loss -  tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  219  Loss -  tensor(0.0634, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  220  Loss -  tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  221  Loss -  tensor(0.0627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  222  Loss -  tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  223  Loss -  tensor(0.0619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  224  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  225  Loss -  tensor(0.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  226  Loss -  tensor(0.0619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  227  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  228  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  229  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  230  Loss -  tensor(0.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  231  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  232  Loss -  tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  233  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  234  Loss -  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  235  Loss -  tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  236  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  237  Loss -  tensor(0.0577, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  238  Loss -  tensor(0.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  239  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  240  Loss -  tensor(0.0640, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  241  Loss -  tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  242  Loss -  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  243  Loss -  tensor(0.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  244  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  245  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  246  Loss -  tensor(0.0602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  247  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  248  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  249  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  250  Loss -  tensor(0.0622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  251  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  252  Loss -  tensor(0.0602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  253  Loss -  tensor(0.0634, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  254  Loss -  tensor(0.0568, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  255  Loss -  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  256  Loss -  tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  257  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  258  Loss -  tensor(0.0576, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  259  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  260  Loss -  tensor(0.0611, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  261  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  262  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  263  Loss -  tensor(0.0566, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  264  Loss -  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  265  Loss -  tensor(0.0624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  266  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  267  Loss -  tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  268  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  269  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  270  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  271  Loss -  tensor(0.0604, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  272  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  273  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  274  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  275  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  276  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  277  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  278  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  279  Loss -  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  280  Loss -  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  281  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  282  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  283  Loss -  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  284  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  285  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  286  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  287  Loss -  tensor(0.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  288  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  289  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  290  Loss -  tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  291  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  292  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  293  Loss -  tensor(0.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  294  Loss -  tensor(0.0586, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  295  Loss -  tensor(0.0641, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  296  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  297  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  298  Loss -  tensor(0.0577, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  299  Loss -  tensor(0.0576, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  300  Loss -  tensor(0.0631, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  301  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  302  Loss -  tensor(0.0566, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  303  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  304  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  305  Loss -  tensor(0.0563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  306  Loss -  tensor(0.0581, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  307  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  308  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  309  Loss -  tensor(0.0575, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  310  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  311  Loss -  tensor(0.0624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  312  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  313  Loss -  tensor(0.0617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  314  Loss -  tensor(0.0581, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  315  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  316  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  317  Loss -  tensor(0.0623, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  318  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  319  Loss -  tensor(0.0576, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  320  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  321  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  322  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  323  Loss -  tensor(0.0577, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  324  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  325  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  326  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  327  Loss -  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  328  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  329  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  330  Loss -  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  331  Loss -  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  332  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  333  Loss -  tensor(0.0624, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  334  Loss -  tensor(0.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  335  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  336  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  337  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  338  Loss -  tensor(0.0581, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  339  Loss -  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  340  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  341  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  342  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  343  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  344  Loss -  tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  345  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  346  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  347  Loss -  tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  348  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  349  Loss -  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  350  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  351  Loss -  tensor(0.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  352  Loss -  tensor(0.0576, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  353  Loss -  tensor(0.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  354  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  355  Loss -  tensor(0.0633, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  356  Loss -  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  357  Loss -  tensor(0.0563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  358  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  359  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  360  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  361  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  362  Loss -  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  363  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  364  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  365  Loss -  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  366  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  367  Loss -  tensor(0.0602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  368  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  369  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  370  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  371  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  372  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  373  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  374  Loss -  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  375  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  376  Loss -  tensor(0.0566, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  377  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  378  Loss -  tensor(0.0906, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -  0  Loss -  tensor(0.0577, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  1  Loss -  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  2  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  3  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  4  Loss -  tensor(0.0577, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  5  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  6  Loss -  tensor(0.0619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  7  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  8  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  9  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  10  Loss -  tensor(0.0575, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  11  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  12  Loss -  tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  13  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  14  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  15  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  16  Loss -  tensor(0.0561, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  17  Loss -  tensor(0.0568, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  18  Loss -  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  19  Loss -  tensor(0.0635, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  20  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  21  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  22  Loss -  tensor(0.0598, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  23  Loss -  tensor(0.0577, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  24  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  25  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  26  Loss -  tensor(0.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  27  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  28  Loss -  tensor(0.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  29  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  30  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  31  Loss -  tensor(0.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  32  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  33  Loss -  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  34  Loss -  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  35  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  36  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  37  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  38  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  39  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  40  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  41  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  42  Loss -  tensor(0.0581, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  43  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  44  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  45  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  46  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  47  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  48  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  49  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  50  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  51  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  52  Loss -  tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  53  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  54  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  55  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  56  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  57  Loss -  tensor(0.0538, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  58  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  59  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  60  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  61  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  62  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  63  Loss -  tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  64  Loss -  tensor(0.0586, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  65  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  66  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  67  Loss -  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  68  Loss -  tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  69  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  70  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  71  Loss -  tensor(0.0614, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  72  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  73  Loss -  tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  74  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  75  Loss -  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  76  Loss -  tensor(0.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  77  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  78  Loss -  tensor(0.0547, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  79  Loss -  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  80  Loss -  tensor(0.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  81  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  82  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  83  Loss -  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  84  Loss -  tensor(0.0547, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  85  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  86  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  87  Loss -  tensor(0.0581, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  88  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  89  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  90  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  91  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  92  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  93  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  94  Loss -  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  95  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  96  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  97  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  98  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  99  Loss -  tensor(0.0568, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  100  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  101  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  102  Loss -  tensor(0.0598, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  103  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  104  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  105  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  106  Loss -  tensor(0.0575, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  107  Loss -  tensor(0.0538, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  108  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  109  Loss -  tensor(0.0566, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  110  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  111  Loss -  tensor(0.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  112  Loss -  tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  113  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  114  Loss -  tensor(0.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  115  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  116  Loss -  tensor(0.0575, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  117  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  118  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  119  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  120  Loss -  tensor(0.0563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  121  Loss -  tensor(0.0576, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  122  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  123  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  124  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  125  Loss -  tensor(0.0620, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  126  Loss -  tensor(0.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  127  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  128  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  129  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  130  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  131  Loss -  tensor(0.0575, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  132  Loss -  tensor(0.0566, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  133  Loss -  tensor(0.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  134  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  135  Loss -  tensor(0.0623, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  136  Loss -  tensor(0.0575, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  137  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  138  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  139  Loss -  tensor(0.0561, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  140  Loss -  tensor(0.0517, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  141  Loss -  tensor(0.0621, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  142  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  143  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  144  Loss -  tensor(0.0561, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  145  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  146  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  147  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  148  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  149  Loss -  tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  150  Loss -  tensor(0.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  151  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  152  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  153  Loss -  tensor(0.0528, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  154  Loss -  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  155  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  156  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  157  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  158  Loss -  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  159  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  160  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  161  Loss -  tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  162  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  163  Loss -  tensor(0.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  164  Loss -  tensor(0.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  165  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  166  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  167  Loss -  tensor(0.0608, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  168  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  169  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  170  Loss -  tensor(0.0613, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  171  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  172  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  173  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  174  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  175  Loss -  tensor(0.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  176  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  177  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  178  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  179  Loss -  tensor(0.0598, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  180  Loss -  tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  181  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  182  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  183  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  184  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  185  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  186  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  187  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  188  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  189  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  190  Loss -  tensor(0.0539, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  191  Loss -  tensor(0.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  192  Loss -  tensor(0.0556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  193  Loss -  tensor(0.0576, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  194  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  195  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  196  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  197  Loss -  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  198  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  199  Loss -  tensor(0.0556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  200  Loss -  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  201  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  202  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  203  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  204  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  205  Loss -  tensor(0.0575, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  206  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  207  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  208  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  209  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  210  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  211  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  212  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  213  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  214  Loss -  tensor(0.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  215  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  216  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  217  Loss -  tensor(0.0575, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  218  Loss -  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  219  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  220  Loss -  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  221  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  222  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  223  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  224  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  225  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  226  Loss -  tensor(0.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  227  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  228  Loss -  tensor(0.0633, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  229  Loss -  tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  230  Loss -  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  231  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  232  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  233  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  234  Loss -  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  235  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  236  Loss -  tensor(0.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  237  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  238  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  239  Loss -  tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  240  Loss -  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  241  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  242  Loss -  tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  243  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  244  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  245  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  246  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  247  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  248  Loss -  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  249  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  250  Loss -  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  251  Loss -  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  252  Loss -  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  253  Loss -  tensor(0.0561, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  254  Loss -  tensor(0.0593, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  255  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  256  Loss -  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  257  Loss -  tensor(0.0561, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  258  Loss -  tensor(0.0615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  259  Loss -  tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  260  Loss -  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  261  Loss -  tensor(0.0557, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  262  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  263  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  264  Loss -  tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  265  Loss -  tensor(0.0575, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  266  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  267  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  268  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  269  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  270  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  271  Loss -  tensor(0.0538, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  272  Loss -  tensor(0.0581, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  273  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  274  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  275  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  276  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  277  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  278  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  279  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  280  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  281  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  282  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  283  Loss -  tensor(0.0568, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  284  Loss -  tensor(0.0575, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  285  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  286  Loss -  tensor(0.0577, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  287  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  288  Loss -  tensor(0.0566, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  289  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  290  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  291  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  292  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  293  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  294  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  295  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  296  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  297  Loss -  tensor(0.0586, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  298  Loss -  tensor(0.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  299  Loss -  tensor(0.0577, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  300  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  301  Loss -  tensor(0.0561, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  302  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  303  Loss -  tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  304  Loss -  tensor(0.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  305  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  306  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  307  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  308  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  309  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  310  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  311  Loss -  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  312  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  313  Loss -  tensor(0.0568, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  314  Loss -  tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  315  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  316  Loss -  tensor(0.0561, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  317  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  318  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  319  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  320  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  321  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  322  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  323  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  324  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  325  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  326  Loss -  tensor(0.0575, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  327  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  328  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  329  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  330  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  331  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  332  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  333  Loss -  tensor(0.0557, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  334  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  335  Loss -  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  336  Loss -  tensor(0.0556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  337  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  338  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  339  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  340  Loss -  tensor(0.0547, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  341  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  342  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  343  Loss -  tensor(0.0577, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  344  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  345  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  346  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  347  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  348  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  349  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  350  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  351  Loss -  tensor(0.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  352  Loss -  tensor(0.0640, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  353  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  354  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  355  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  356  Loss -  tensor(0.0561, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  357  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  358  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  359  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  360  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  361  Loss -  tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  362  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  363  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  364  Loss -  tensor(0.0556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  365  Loss -  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  366  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  367  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  368  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  369  Loss -  tensor(0.0604, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  370  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  371  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  372  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  373  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  374  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  375  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  376  Loss -  tensor(0.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  377  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  378  Loss -  tensor(0.0909, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -  0  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  1  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  2  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  3  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  4  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  5  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  6  Loss -  tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  7  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  8  Loss -  tensor(0.0566, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  9  Loss -  tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  10  Loss -  tensor(0.0576, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  11  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  12  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  13  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  14  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  15  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  16  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  17  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  18  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  19  Loss -  tensor(0.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  20  Loss -  tensor(0.0576, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  21  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  22  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  23  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  24  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  25  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  26  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  27  Loss -  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  28  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  29  Loss -  tensor(0.0557, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  30  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  31  Loss -  tensor(0.0561, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  32  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  33  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  34  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  35  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  36  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  37  Loss -  tensor(0.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  38  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  39  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  40  Loss -  tensor(0.0600, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  41  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  42  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  43  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  44  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  45  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  46  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  47  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  48  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  49  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  50  Loss -  tensor(0.0563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  51  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  52  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  53  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  54  Loss -  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  55  Loss -  tensor(0.0608, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  56  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  57  Loss -  tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  58  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  59  Loss -  tensor(0.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  60  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  61  Loss -  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  62  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  63  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  64  Loss -  tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  65  Loss -  tensor(0.0577, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  66  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  67  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  68  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  69  Loss -  tensor(0.0561, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  70  Loss -  tensor(0.0563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  71  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  72  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  73  Loss -  tensor(0.0557, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  74  Loss -  tensor(0.0608, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  75  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  76  Loss -  tensor(0.0561, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  77  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  78  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  79  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  80  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  81  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  82  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  83  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  84  Loss -  tensor(0.0604, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  85  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  86  Loss -  tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  87  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  88  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  89  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  90  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  91  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  92  Loss -  tensor(0.0586, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  93  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  94  Loss -  tensor(0.0586, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  95  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  96  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  97  Loss -  tensor(0.0538, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  98  Loss -  tensor(0.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  99  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  100  Loss -  tensor(0.0563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  101  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  102  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  103  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  104  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  105  Loss -  tensor(0.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  106  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  107  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  108  Loss -  tensor(0.0557, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  109  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  110  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  111  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  112  Loss -  tensor(0.0577, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  113  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  114  Loss -  tensor(0.0586, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  115  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  116  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  117  Loss -  tensor(0.0557, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  118  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  119  Loss -  tensor(0.0566, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  120  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  121  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  122  Loss -  tensor(0.0566, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  123  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  124  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  125  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  126  Loss -  tensor(0.0566, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  127  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  128  Loss -  tensor(0.0602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  129  Loss -  tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  130  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  131  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  132  Loss -  tensor(0.0557, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  133  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  134  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  135  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  136  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  137  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  138  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  139  Loss -  tensor(0.0538, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  140  Loss -  tensor(0.0547, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  141  Loss -  tensor(0.0598, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  142  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  143  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  144  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  145  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  146  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  147  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  148  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  149  Loss -  tensor(0.0563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  150  Loss -  tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  151  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  152  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  153  Loss -  tensor(0.0586, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  154  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  155  Loss -  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  156  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  157  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  158  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  159  Loss -  tensor(0.0581, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  160  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  161  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  162  Loss -  tensor(0.0566, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  163  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  164  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  165  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  166  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  167  Loss -  tensor(0.0563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  168  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  169  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  170  Loss -  tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  171  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  172  Loss -  tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  173  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  174  Loss -  tensor(0.0566, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  175  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  176  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  177  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  178  Loss -  tensor(0.0598, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  179  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  180  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  181  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  182  Loss -  tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  183  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  184  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  185  Loss -  tensor(0.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  186  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  187  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  188  Loss -  tensor(0.0547, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  189  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  190  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  191  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  192  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  193  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  194  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  195  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  196  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  197  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  198  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  199  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  200  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  201  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  202  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  203  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  204  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  205  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  206  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  207  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  208  Loss -  tensor(0.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  209  Loss -  tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  210  Loss -  tensor(0.0575, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  211  Loss -  tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  212  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  213  Loss -  tensor(0.0602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  214  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  215  Loss -  tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  216  Loss -  tensor(0.0539, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  217  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  218  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  219  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  220  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  221  Loss -  tensor(0.0539, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  222  Loss -  tensor(0.0591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  223  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  224  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  225  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  226  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  227  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  228  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  229  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  230  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  231  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  232  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  233  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  234  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  235  Loss -  tensor(0.0613, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  236  Loss -  tensor(0.0517, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  237  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  238  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  239  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  240  Loss -  tensor(0.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  241  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  242  Loss -  tensor(0.0563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  243  Loss -  tensor(0.0547, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  244  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  245  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  246  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  247  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  248  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  249  Loss -  tensor(0.0597, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  250  Loss -  tensor(0.0598, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  251  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  252  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  253  Loss -  tensor(0.0611, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  254  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  255  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  256  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  257  Loss -  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  258  Loss -  tensor(0.0563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  259  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  260  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  261  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  262  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  263  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  264  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  265  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  266  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  267  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  268  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  269  Loss -  tensor(0.0517, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  270  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  271  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  272  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  273  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  274  Loss -  tensor(0.0568, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  275  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  276  Loss -  tensor(0.0586, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  277  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  278  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  279  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  280  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  281  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  282  Loss -  tensor(0.0566, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  283  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  284  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  285  Loss -  tensor(0.0590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  286  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  287  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  288  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  289  Loss -  tensor(0.0556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  290  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  291  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  292  Loss -  tensor(0.0547, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  293  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  294  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  295  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  296  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  297  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  298  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  299  Loss -  tensor(0.0539, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  300  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  301  Loss -  tensor(0.0539, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  302  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  303  Loss -  tensor(0.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  304  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  305  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  306  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  307  Loss -  tensor(0.0539, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  308  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  309  Loss -  tensor(0.0539, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  310  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  311  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  312  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  313  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  314  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  315  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  316  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  317  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  318  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  319  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  320  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  321  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  322  Loss -  tensor(0.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  323  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  324  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  325  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  326  Loss -  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  327  Loss -  tensor(0.0561, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  328  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  329  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  330  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  331  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  332  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  333  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  334  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  335  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  336  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  337  Loss -  tensor(0.0561, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  338  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  339  Loss -  tensor(0.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  340  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  341  Loss -  tensor(0.0586, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  342  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  343  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  344  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  345  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  346  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  347  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  348  Loss -  tensor(0.0538, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  349  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  350  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  351  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  352  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  353  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  354  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  355  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  356  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  357  Loss -  tensor(0.0556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  358  Loss -  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  359  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  360  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  361  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  362  Loss -  tensor(0.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  363  Loss -  tensor(0.0485, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  364  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  365  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  366  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  367  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  368  Loss -  tensor(0.0557, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  369  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  370  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  371  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  372  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  373  Loss -  tensor(0.0556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  374  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  375  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  376  Loss -  tensor(0.0547, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  377  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  378  Loss -  tensor(0.0879, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -  0  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  1  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  2  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  3  Loss -  tensor(0.0563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  4  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  5  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  6  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  7  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  8  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  9  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  10  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  11  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  12  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  13  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  14  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  15  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  16  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  17  Loss -  tensor(0.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  18  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  19  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  20  Loss -  tensor(0.0528, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  21  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  22  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  23  Loss -  tensor(0.0538, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  24  Loss -  tensor(0.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  25  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  26  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  27  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  28  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  29  Loss -  tensor(0.0575, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  30  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  31  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  32  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  33  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  34  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  35  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  36  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  37  Loss -  tensor(0.0566, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  38  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  39  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  40  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  41  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  42  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  43  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  44  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  45  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  46  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  47  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  48  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  49  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  50  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  51  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  52  Loss -  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  53  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  54  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  55  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  56  Loss -  tensor(0.0584, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  57  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  58  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  59  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  60  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  61  Loss -  tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  62  Loss -  tensor(0.0547, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  63  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  64  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  65  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  66  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  67  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  68  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  69  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  70  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  71  Loss -  tensor(0.0619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  72  Loss -  tensor(0.0556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  73  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  74  Loss -  tensor(0.0581, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  75  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  76  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  77  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  78  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  79  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  80  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  81  Loss -  tensor(0.0556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  82  Loss -  tensor(0.0539, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  83  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  84  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  85  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  86  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  87  Loss -  tensor(0.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  88  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  89  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  90  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  91  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  92  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  93  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  94  Loss -  tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  95  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  96  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  97  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  98  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  99  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  100  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  101  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  102  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  103  Loss -  tensor(0.0563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  104  Loss -  tensor(0.0556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  105  Loss -  tensor(0.0576, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  106  Loss -  tensor(0.0576, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  107  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  108  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  109  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  110  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  111  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  112  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  113  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  114  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  115  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  116  Loss -  tensor(0.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  117  Loss -  tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  118  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  119  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  120  Loss -  tensor(0.0581, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  121  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  122  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  123  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  124  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  125  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  126  Loss -  tensor(0.0568, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  127  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  128  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  129  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  130  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  131  Loss -  tensor(0.0556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  132  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  133  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  134  Loss -  tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  135  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  136  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  137  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  138  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  139  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  140  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  141  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  142  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  143  Loss -  tensor(0.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  144  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  145  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  146  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  147  Loss -  tensor(0.0538, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  148  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  149  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  150  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  151  Loss -  tensor(0.0557, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  152  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  153  Loss -  tensor(0.0576, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  154  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  155  Loss -  tensor(0.0517, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  156  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  157  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  158  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  159  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  160  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  161  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  162  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  163  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  164  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  165  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  166  Loss -  tensor(0.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  167  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  168  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  169  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  170  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  171  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  172  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  173  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  174  Loss -  tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  175  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  176  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  177  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  178  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  179  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  180  Loss -  tensor(0.0547, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  181  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  182  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  183  Loss -  tensor(0.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  184  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  185  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  186  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  187  Loss -  tensor(0.0528, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  188  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  189  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  190  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  191  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  192  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  193  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  194  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  195  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  196  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  197  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  198  Loss -  tensor(0.0575, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  199  Loss -  tensor(0.0557, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  200  Loss -  tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  201  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  202  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  203  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  204  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  205  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  206  Loss -  tensor(0.0528, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  207  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  208  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  209  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  210  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  211  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  212  Loss -  tensor(0.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  213  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  214  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  215  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  216  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  217  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  218  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  219  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  220  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  221  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  222  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  223  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  224  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  225  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  226  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  227  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  228  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  229  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  230  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  231  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  232  Loss -  tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  233  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  234  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  235  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  236  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  237  Loss -  tensor(0.0485, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  238  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  239  Loss -  tensor(0.0561, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  240  Loss -  tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  241  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  242  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  243  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  244  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  245  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  246  Loss -  tensor(0.0485, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  247  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  248  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  249  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  250  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  251  Loss -  tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  252  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  253  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  254  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  255  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  256  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  257  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  258  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  259  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  260  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  261  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  262  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  263  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  264  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  265  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  266  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  267  Loss -  tensor(0.0539, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  268  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  269  Loss -  tensor(0.0539, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  270  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  271  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  272  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  273  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  274  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  275  Loss -  tensor(0.0585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  276  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  277  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  278  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  279  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  280  Loss -  tensor(0.0563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  281  Loss -  tensor(0.0547, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  282  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  283  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  284  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  285  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  286  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  287  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  288  Loss -  tensor(0.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  289  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  290  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  291  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  292  Loss -  tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  293  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  294  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  295  Loss -  tensor(0.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  296  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  297  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  298  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  299  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  300  Loss -  tensor(0.0517, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  301  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  302  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  303  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  304  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  305  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  306  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  307  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  308  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  309  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  310  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  311  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  312  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  313  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  314  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  315  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  316  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  317  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  318  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  319  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  320  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  321  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  322  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  323  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  324  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  325  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  326  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  327  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  328  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  329  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  330  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  331  Loss -  tensor(0.0579, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  332  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  333  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  334  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  335  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  336  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  337  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  338  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  339  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  340  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  341  Loss -  tensor(0.0558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  342  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  343  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  344  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  345  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  346  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  347  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  348  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  349  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  350  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  351  Loss -  tensor(0.0538, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  352  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  353  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  354  Loss -  tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  355  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  356  Loss -  tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  357  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  358  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  359  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  360  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  361  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  362  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  363  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  364  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  365  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  366  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  367  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  368  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  369  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  370  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  371  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  372  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  373  Loss -  tensor(0.0517, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  374  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  375  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  376  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  377  Loss -  tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  378  Loss -  tensor(0.0822, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -  0  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  1  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  2  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  3  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  4  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  5  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  6  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  7  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  8  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  9  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  10  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  11  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  12  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  13  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  14  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  15  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  16  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  17  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  18  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  19  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  20  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  21  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  22  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  23  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  24  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  25  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  26  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  27  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  28  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  29  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  30  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  31  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  32  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  33  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  34  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  35  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  36  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  37  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  38  Loss -  tensor(0.0485, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  39  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  40  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  41  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  42  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  43  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  44  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  45  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  46  Loss -  tensor(0.0582, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  47  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  48  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  49  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  50  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  51  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  52  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  53  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  54  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  55  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  56  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  57  Loss -  tensor(0.0517, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  58  Loss -  tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  59  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  60  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  61  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  62  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  63  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  64  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  65  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  66  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  67  Loss -  tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  68  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  69  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  70  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  71  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  72  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  73  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  74  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  75  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  76  Loss -  tensor(0.0561, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  77  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  78  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  79  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  80  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  81  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  82  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  83  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  84  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  85  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  86  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  87  Loss -  tensor(0.0539, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  88  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  89  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  90  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  91  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  92  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  93  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  94  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  95  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  96  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  97  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  98  Loss -  tensor(0.0563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  99  Loss -  tensor(0.0485, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  100  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  101  Loss -  tensor(0.0517, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  102  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  103  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  104  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  105  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  106  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  107  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  108  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  109  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  110  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  111  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  112  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  113  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  114  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  115  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  116  Loss -  tensor(0.0547, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  117  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  118  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  119  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  120  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  121  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  122  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  123  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  124  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  125  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  126  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  127  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  128  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  129  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  130  Loss -  tensor(0.0557, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  131  Loss -  tensor(0.0547, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  132  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  133  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  134  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  135  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  136  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  137  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  138  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  139  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  140  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  141  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  142  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  143  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  144  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  145  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  146  Loss -  tensor(0.0517, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  147  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  148  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  149  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  150  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  151  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  152  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  153  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  154  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  155  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  156  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  157  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  158  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  159  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  160  Loss -  tensor(0.0517, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  161  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  162  Loss -  tensor(0.0565, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  163  Loss -  tensor(0.0556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  164  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  165  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  166  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  167  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  168  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  169  Loss -  tensor(0.0539, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  170  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  171  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  172  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  173  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  174  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  175  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  176  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  177  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  178  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  179  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  180  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  181  Loss -  tensor(0.0562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  182  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  183  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  184  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  185  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  186  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  187  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  188  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  189  Loss -  tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  190  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  191  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  192  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  193  Loss -  tensor(0.0557, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  194  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  195  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  196  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  197  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  198  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  199  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  200  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  201  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  202  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  203  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  204  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  205  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  206  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  207  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  208  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  209  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  210  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  211  Loss -  tensor(0.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  212  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  213  Loss -  tensor(0.0539, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  214  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  215  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  216  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  217  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  218  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  219  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  220  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  221  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  222  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  223  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  224  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  225  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  226  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  227  Loss -  tensor(0.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  228  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  229  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  230  Loss -  tensor(0.0528, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  231  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  232  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  233  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  234  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  235  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  236  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  237  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  238  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  239  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  240  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  241  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  242  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  243  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  244  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  245  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  246  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  247  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  248  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  249  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  250  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  251  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  252  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  253  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  254  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  255  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  256  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  257  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  258  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  259  Loss -  tensor(0.0556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  260  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  261  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  262  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  263  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  264  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  265  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  266  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  267  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  268  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  269  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  270  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  271  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  272  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  273  Loss -  tensor(0.0563, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  274  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  275  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  276  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  277  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  278  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  279  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  280  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  281  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  282  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  283  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  284  Loss -  tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  285  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  286  Loss -  tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  287  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  288  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  289  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  290  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  291  Loss -  tensor(0.0570, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  292  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  293  Loss -  tensor(0.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  294  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  295  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  296  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  297  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  298  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  299  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  300  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  301  Loss -  tensor(0.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  302  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  303  Loss -  tensor(0.0550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  304  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  305  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  306  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  307  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  308  Loss -  tensor(0.0583, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  309  Loss -  tensor(0.0528, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  310  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  311  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  312  Loss -  tensor(0.0571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  313  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  314  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  315  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  316  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  317  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  318  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  319  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  320  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  321  Loss -  tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  322  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  323  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  324  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  325  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  326  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  327  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  328  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  329  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  330  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  331  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  332  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  333  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  334  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  335  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  336  Loss -  tensor(0.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  337  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  338  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  339  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  340  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  341  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  342  Loss -  tensor(0.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  343  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  344  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  345  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  346  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  347  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  348  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  349  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  350  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  351  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  352  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  353  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  354  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  355  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  356  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  357  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  358  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  359  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  360  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  361  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  362  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  363  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  364  Loss -  tensor(0.0538, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  365  Loss -  tensor(0.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  366  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  367  Loss -  tensor(0.0545, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  368  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  369  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  370  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  371  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  372  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  373  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  374  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  375  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  376  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  377  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  378  Loss -  tensor(0.0865, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -  0  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  1  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  2  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  3  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  4  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  5  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  6  Loss -  tensor(0.0557, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  7  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  8  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  9  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  10  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  11  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  12  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  13  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  14  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  15  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  16  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  17  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  18  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  19  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  20  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  21  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  22  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  23  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  24  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  25  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  26  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  27  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  28  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  29  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  30  Loss -  tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  31  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  32  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  33  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  34  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  35  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  36  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  37  Loss -  tensor(0.0574, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  38  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  39  Loss -  tensor(0.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  40  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  41  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  42  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  43  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  44  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  45  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  46  Loss -  tensor(0.0547, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  47  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  48  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  49  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  50  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  51  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  52  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  53  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  54  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  55  Loss -  tensor(0.0556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  56  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  57  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  58  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  59  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  60  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  61  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  62  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  63  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  64  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  65  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  66  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  67  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  68  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  69  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  70  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  71  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  72  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  73  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  74  Loss -  tensor(0.0538, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  75  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  76  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  77  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  78  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  79  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  80  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  81  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  82  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  83  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  84  Loss -  tensor(0.0528, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  85  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  86  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  87  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  88  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  89  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  90  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  91  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  92  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  93  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  94  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  95  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  96  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  97  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  98  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  99  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  100  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  101  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  102  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  103  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  104  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  105  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  106  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  107  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  108  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  109  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  110  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  111  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  112  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  113  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  114  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  115  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  116  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  117  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  118  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  119  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  120  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  121  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  122  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  123  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  124  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  125  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  126  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  127  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  128  Loss -  tensor(0.0538, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  129  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  130  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  131  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  132  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  133  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  134  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  135  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  136  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  137  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  138  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  139  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  140  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  141  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  142  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  143  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  144  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  145  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  146  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  147  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  148  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  149  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  150  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  151  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  152  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  153  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  154  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  155  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  156  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  157  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  158  Loss -  tensor(0.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  159  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  160  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  161  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  162  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  163  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  164  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  165  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  166  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  167  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  168  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  169  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  170  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  171  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  172  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  173  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  174  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  175  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  176  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  177  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  178  Loss -  tensor(0.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  179  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  180  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  181  Loss -  tensor(0.0485, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  182  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  183  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  184  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  185  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  186  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  187  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  188  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  189  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  190  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  191  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  192  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  193  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  194  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  195  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  196  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  197  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  198  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  199  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  200  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  201  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  202  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  203  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  204  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  205  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  206  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  207  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  208  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  209  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  210  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  211  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  212  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  213  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  214  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  215  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  216  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  217  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  218  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  219  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  220  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  221  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  222  Loss -  tensor(0.0528, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  223  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  224  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  225  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  226  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  227  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  228  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  229  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  230  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  231  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  232  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  233  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  234  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  235  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  236  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  237  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  238  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  239  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  240  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  241  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  242  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  243  Loss -  tensor(0.0528, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  244  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  245  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  246  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  247  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  248  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  249  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  250  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  251  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  252  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  253  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  254  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  255  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  256  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  257  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  258  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  259  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  260  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  261  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  262  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  263  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  264  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  265  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  266  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  267  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  268  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  269  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  270  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  271  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  272  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  273  Loss -  tensor(0.0517, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  274  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  275  Loss -  tensor(0.0528, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  276  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  277  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  278  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  279  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  280  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  281  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  282  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  283  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  284  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  285  Loss -  tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  286  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  287  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  288  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  289  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  290  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  291  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  292  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  293  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  294  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  295  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  296  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  297  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  298  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  299  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  300  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  301  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  302  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  303  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  304  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  305  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  306  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  307  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  308  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  309  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  310  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  311  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  312  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  313  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  314  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  315  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  316  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  317  Loss -  tensor(0.0538, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  318  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  319  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  320  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  321  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  322  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  323  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  324  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  325  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  326  Loss -  tensor(0.0485, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  327  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  328  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  329  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  330  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  331  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  332  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  333  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  334  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  335  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  336  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  337  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  338  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  339  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  340  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  341  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  342  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  343  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  344  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  345  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  346  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  347  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  348  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  349  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  350  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  351  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  352  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  353  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  354  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  355  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  356  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  357  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  358  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  359  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  360  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  361  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  362  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  363  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  364  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  365  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  366  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  367  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  368  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  369  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  370  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  371  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  372  Loss -  tensor(0.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  373  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  374  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  375  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  376  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  377  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  378  Loss -  tensor(0.0713, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -  0  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  1  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  2  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  3  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  4  Loss -  tensor(0.0485, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  5  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  6  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  7  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  8  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  9  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  10  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  11  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  12  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  13  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  14  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  15  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  16  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  17  Loss -  tensor(0.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  18  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  19  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  20  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  21  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  22  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  23  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  24  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  25  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  26  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  27  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  28  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  29  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  30  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  31  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  32  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  33  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  34  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  35  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  36  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  37  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  38  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  39  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  40  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  41  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  42  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  43  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  44  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  45  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  46  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  47  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  48  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  49  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  50  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  51  Loss -  tensor(0.0517, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  52  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  53  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  54  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  55  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  56  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  57  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  58  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  59  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  60  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  61  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  62  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  63  Loss -  tensor(0.0517, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  64  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  65  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  66  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  67  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  68  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  69  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  70  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  71  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  72  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  73  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  74  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  75  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  76  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  77  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  78  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  79  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  80  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  81  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  82  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  83  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  84  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  85  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  86  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  87  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  88  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  89  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  90  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  91  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  92  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  93  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  94  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  95  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  96  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  97  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  98  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  99  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  100  Loss -  tensor(0.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  101  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  102  Loss -  tensor(0.0525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  103  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  104  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  105  Loss -  tensor(0.0560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  106  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  107  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  108  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  109  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  110  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  111  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  112  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  113  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  114  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  115  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  116  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  117  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  118  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  119  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  120  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  121  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  122  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  123  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  124  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  125  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  126  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  127  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  128  Loss -  tensor(0.0541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  129  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  130  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  131  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  132  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  133  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  134  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  135  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  136  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  137  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  138  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  139  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  140  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  141  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  142  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  143  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  144  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  145  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  146  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  147  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  148  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  149  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  150  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  151  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  152  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  153  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  154  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  155  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  156  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  157  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  158  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  159  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  160  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  161  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  162  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  163  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  164  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  165  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  166  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  167  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  168  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  169  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  170  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  171  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  172  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  173  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  174  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  175  Loss -  tensor(0.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  176  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  177  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  178  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  179  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  180  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  181  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  182  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  183  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  184  Loss -  tensor(0.0546, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  185  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  186  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  187  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  188  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  189  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  190  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  191  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  192  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  193  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  194  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  195  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  196  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  197  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  198  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  199  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  200  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  201  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  202  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  203  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  204  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  205  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  206  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  207  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  208  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  209  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  210  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  211  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  212  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  213  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  214  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  215  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  216  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  217  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  218  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  219  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  220  Loss -  tensor(0.0553, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  221  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  222  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  223  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  224  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  225  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  226  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  227  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  228  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  229  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  230  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  231  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  232  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  233  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  234  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  235  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  236  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  237  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  238  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  239  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  240  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  241  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  242  Loss -  tensor(0.0528, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  243  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  244  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  245  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  246  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  247  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  248  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  249  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  250  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  251  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  252  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  253  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  254  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  255  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  256  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  257  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  258  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  259  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  260  Loss -  tensor(0.0539, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  261  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  262  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  263  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  264  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  265  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  266  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  267  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  268  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  269  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  270  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  271  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  272  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  273  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  274  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  275  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  276  Loss -  tensor(0.0551, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  277  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  278  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  279  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  280  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  281  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  282  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  283  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  284  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  285  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  286  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  287  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  288  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  289  Loss -  tensor(0.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  290  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  291  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  292  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  293  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  294  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  295  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  296  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  297  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  298  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  299  Loss -  tensor(0.0537, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  300  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  301  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  302  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  303  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  304  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  305  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  306  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  307  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  308  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  309  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  310  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  311  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  312  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  313  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  314  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  315  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  316  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  317  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  318  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  319  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  320  Loss -  tensor(0.0517, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  321  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  322  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  323  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  324  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  325  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  326  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  327  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  328  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  329  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  330  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  331  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  332  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  333  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  334  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  335  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  336  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  337  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  338  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  339  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  340  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  341  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  342  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  343  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  344  Loss -  tensor(0.0517, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  345  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  346  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  347  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  348  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  349  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  350  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  351  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  352  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  353  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  354  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  355  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  356  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  357  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  358  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  359  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  360  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  361  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  362  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  363  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  364  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  365  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  366  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  367  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  368  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  369  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  370  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  371  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  372  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  373  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  374  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  375  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  376  Loss -  tensor(0.0485, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  377  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  378  Loss -  tensor(0.0771, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -  0  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  1  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  2  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  3  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  4  Loss -  tensor(0.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  5  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  6  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  7  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  8  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  9  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  10  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  11  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  12  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  13  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  14  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  15  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  16  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  17  Loss -  tensor(0.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  18  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  19  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  20  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  21  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  22  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  23  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  24  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  25  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  26  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  27  Loss -  tensor(0.0521, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  28  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  29  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  30  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  31  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  32  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  33  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  34  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  35  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  36  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  37  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  38  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  39  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  40  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  41  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  42  Loss -  tensor(0.0534, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  43  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  44  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  45  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  46  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  47  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  48  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  49  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  50  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  51  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  52  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  53  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  54  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  55  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  56  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  57  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  58  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  59  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  60  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  61  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  62  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  63  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  64  Loss -  tensor(0.0526, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  65  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  66  Loss -  tensor(0.0485, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  67  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  68  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  69  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  70  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  71  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  72  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  73  Loss -  tensor(0.0540, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  74  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  75  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  76  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  77  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  78  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  79  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  80  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  81  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  82  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  83  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  84  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  85  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  86  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  87  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  88  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  89  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  90  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  91  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  92  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  93  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  94  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  95  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  96  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  97  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  98  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  99  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  100  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  101  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  102  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  103  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  104  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  105  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  106  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  107  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  108  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  109  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  110  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  111  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  112  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  113  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  114  Loss -  tensor(0.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  115  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  116  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  117  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  118  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  119  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  120  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  121  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  122  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  123  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  124  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  125  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  126  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  127  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  128  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  129  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  130  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  131  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  132  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  133  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  134  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  135  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  136  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  137  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  138  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  139  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  140  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  141  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  142  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  143  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  144  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  145  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  146  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  147  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  148  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  149  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  150  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  151  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  152  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  153  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  154  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  155  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  156  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  157  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  158  Loss -  tensor(0.0485, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  159  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  160  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  161  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  162  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  163  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  164  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  165  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  166  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  167  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  168  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  169  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  170  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  171  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  172  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  173  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  174  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  175  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  176  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  177  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  178  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  179  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  180  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  181  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  182  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  183  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  184  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  185  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  186  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  187  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  188  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  189  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  190  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  191  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  192  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  193  Loss -  tensor(0.0517, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  194  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  195  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  196  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  197  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  198  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  199  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  200  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  201  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  202  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  203  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  204  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  205  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  206  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  207  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  208  Loss -  tensor(0.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  209  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  210  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  211  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  212  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  213  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  214  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  215  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  216  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  217  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  218  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  219  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  220  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  221  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  222  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  223  Loss -  tensor(0.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  224  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  225  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  226  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  227  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  228  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  229  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  230  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  231  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  232  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  233  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  234  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  235  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  236  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  237  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  238  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  239  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  240  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  241  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  242  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  243  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  244  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  245  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  246  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  247  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  248  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  249  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  250  Loss -  tensor(0.0536, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  251  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  252  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  253  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  254  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  255  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  256  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  257  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  258  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  259  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  260  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  261  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  262  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  263  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  264  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  265  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  266  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  267  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  268  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  269  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  270  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  271  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  272  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  273  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  274  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  275  Loss -  tensor(0.0401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  276  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  277  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  278  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  279  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  280  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  281  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  282  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  283  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  284  Loss -  tensor(0.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  285  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  286  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  287  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  288  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  289  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  290  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  291  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  292  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  293  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  294  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  295  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  296  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  297  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  298  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  299  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  300  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  301  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  302  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  303  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  304  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  305  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  306  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  307  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  308  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  309  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  310  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  311  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  312  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  313  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  314  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  315  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  316  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  317  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  318  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  319  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  320  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  321  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  322  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  323  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  324  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  325  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  326  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  327  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  328  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  329  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  330  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  331  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  332  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  333  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  334  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  335  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  336  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  337  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  338  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  339  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  340  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  341  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  342  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  343  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  344  Loss -  tensor(0.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  345  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  346  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  347  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  348  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  349  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  350  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  351  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  352  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  353  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  354  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  355  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  356  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  357  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  358  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  359  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  360  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  361  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  362  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  363  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  364  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  365  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  366  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  367  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  368  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  369  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  370  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  371  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  372  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  373  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  374  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  375  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  376  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  377  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  378  Loss -  tensor(0.0816, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -  0  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  1  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  2  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  3  Loss -  tensor(0.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  4  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  5  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  6  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  7  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  8  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  9  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  10  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  11  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  12  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  13  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  14  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  15  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  16  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  17  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  18  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  19  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  20  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  21  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  22  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  23  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  24  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  25  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  26  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  27  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  28  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  29  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  30  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  31  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  32  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  33  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  34  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  35  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  36  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  37  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  38  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  39  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  40  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  41  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  42  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  43  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  44  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  45  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  46  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  47  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  48  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  49  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  50  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  51  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  52  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  53  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  54  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  55  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  56  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  57  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  58  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  59  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  60  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  61  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  62  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  63  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  64  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  65  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  66  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  67  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  68  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  69  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  70  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  71  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  72  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  73  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  74  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  75  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  76  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  77  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  78  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  79  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  80  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  81  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  82  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  83  Loss -  tensor(0.0485, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  84  Loss -  tensor(0.0520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  85  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  86  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  87  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  88  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  89  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  90  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  91  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  92  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  93  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  94  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  95  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  96  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  97  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  98  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  99  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  100  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  101  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  102  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  103  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  104  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  105  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  106  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  107  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  108  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  109  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  110  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  111  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  112  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  113  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  114  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  115  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  116  Loss -  tensor(0.0538, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  117  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  118  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  119  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  120  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  121  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  122  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  123  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  124  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  125  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  126  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  127  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  128  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  129  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  130  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  131  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  132  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  133  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  134  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  135  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  136  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  137  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  138  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  139  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  140  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  141  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  142  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  143  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  144  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  145  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  146  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  147  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  148  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  149  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  150  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  151  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  152  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  153  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  154  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  155  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  156  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  157  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  158  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  159  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  160  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  161  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  162  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  163  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  164  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  165  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  166  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  167  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  168  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  169  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  170  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  171  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  172  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  173  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  174  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  175  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  176  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  177  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  178  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  179  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  180  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  181  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  182  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  183  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  184  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  185  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  186  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  187  Loss -  tensor(0.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  188  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  189  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  190  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  191  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  192  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  193  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  194  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  195  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  196  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  197  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  198  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  199  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  200  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  201  Loss -  tensor(0.0485, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  202  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  203  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  204  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  205  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  206  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  207  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  208  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  209  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  210  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  211  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  212  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  213  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  214  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  215  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  216  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  217  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  218  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  219  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  220  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  221  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  222  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  223  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  224  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  225  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  226  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  227  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  228  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  229  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  230  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  231  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  232  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  233  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  234  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  235  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  236  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  237  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  238  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  239  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  240  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  241  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  242  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  243  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  244  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  245  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  246  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  247  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  248  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  249  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  250  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  251  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  252  Loss -  tensor(0.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  253  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  254  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  255  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  256  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  257  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  258  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  259  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  260  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  261  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  262  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  263  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  264  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  265  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  266  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  267  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  268  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  269  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  270  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  271  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  272  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  273  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  274  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  275  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  276  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  277  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  278  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  279  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  280  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  281  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  282  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  283  Loss -  tensor(0.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  284  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  285  Loss -  tensor(0.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  286  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  287  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  288  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  289  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  290  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  291  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  292  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  293  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  294  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  295  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  296  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  297  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  298  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  299  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  300  Loss -  tensor(0.0384, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  301  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  302  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  303  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  304  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  305  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  306  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  307  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  308  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  309  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  310  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  311  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  312  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  313  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  314  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  315  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  316  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  317  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  318  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  319  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  320  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  321  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  322  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  323  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  324  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  325  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  326  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  327  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  328  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  329  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  330  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  331  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  332  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  333  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  334  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  335  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  336  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  337  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  338  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  339  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  340  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  341  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  342  Loss -  tensor(0.0519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  343  Loss -  tensor(0.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  344  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  345  Loss -  tensor(0.0523, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  346  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  347  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  348  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  349  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  350  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  351  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  352  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  353  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  354  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  355  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  356  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  357  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  358  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  359  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  360  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  361  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  362  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  363  Loss -  tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  364  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  365  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  366  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  367  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  368  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  369  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  370  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  371  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  372  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  373  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  374  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  375  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  376  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  377  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  378  Loss -  tensor(0.0722, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -  0  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  1  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  2  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  3  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  4  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  5  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  6  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  7  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  8  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  9  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  10  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  11  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  12  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  13  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  14  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  15  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  16  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  17  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  18  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  19  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  20  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  21  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  22  Loss -  tensor(0.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  23  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  24  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  25  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  26  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  27  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  28  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  29  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  30  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  31  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  32  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  33  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  34  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  35  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  36  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  37  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  38  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  39  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  40  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  41  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  42  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  43  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  44  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  45  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  46  Loss -  tensor(0.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  47  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  48  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  49  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  50  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  51  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  52  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  53  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  54  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  55  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  56  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  57  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  58  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  59  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  60  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  61  Loss -  tensor(0.0517, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  62  Loss -  tensor(0.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  63  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  64  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  65  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  66  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  67  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  68  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  69  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  70  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  71  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  72  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  73  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  74  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  75  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  76  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  77  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  78  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  79  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  80  Loss -  tensor(0.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  81  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  82  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  83  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  84  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  85  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  86  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  87  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  88  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  89  Loss -  tensor(0.0374, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  90  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  91  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  92  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  93  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  94  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  95  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  96  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  97  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  98  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  99  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  100  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  101  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  102  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  103  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  104  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  105  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  106  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  107  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  108  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  109  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  110  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  111  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  112  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  113  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  114  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  115  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  116  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  117  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  118  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  119  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  120  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  121  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  122  Loss -  tensor(0.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  123  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  124  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  125  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  126  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  127  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  128  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  129  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  130  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  131  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  132  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  133  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  134  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  135  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  136  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  137  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  138  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  139  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  140  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  141  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  142  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  143  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  144  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  145  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  146  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  147  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  148  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  149  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  150  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  151  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  152  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  153  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  154  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  155  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  156  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  157  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  158  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  159  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  160  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  161  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  162  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  163  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  164  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  165  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  166  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  167  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  168  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  169  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  170  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  171  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  172  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  173  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  174  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  175  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  176  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  177  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  178  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  179  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  180  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  181  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  182  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  183  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  184  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  185  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  186  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  187  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  188  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  189  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  190  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  191  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  192  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  193  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  194  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  195  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  196  Loss -  tensor(0.0485, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  197  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  198  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  199  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  200  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  201  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  202  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  203  Loss -  tensor(0.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  204  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  205  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  206  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  207  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  208  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  209  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  210  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  211  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  212  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  213  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  214  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  215  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  216  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  217  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  218  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  219  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  220  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  221  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  222  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  223  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  224  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  225  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  226  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  227  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  228  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  229  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  230  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  231  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  232  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  233  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  234  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  235  Loss -  tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  236  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  237  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  238  Loss -  tensor(0.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  239  Loss -  tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  240  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  241  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  242  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  243  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  244  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  245  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  246  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  247  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  248  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  249  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  250  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  251  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  252  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  253  Loss -  tensor(0.0499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  254  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  255  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  256  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  257  Loss -  tensor(0.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  258  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  259  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  260  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  261  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  262  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  263  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  264  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  265  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  266  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  267  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  268  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  269  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  270  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  271  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  272  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  273  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  274  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  275  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  276  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  277  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  278  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  279  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  280  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  281  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  282  Loss -  tensor(0.0503, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  283  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  284  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  285  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  286  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  287  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  288  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  289  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  290  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  291  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  292  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  293  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  294  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  295  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  296  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  297  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  298  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  299  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  300  Loss -  tensor(0.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  301  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  302  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  303  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  304  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  305  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  306  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  307  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  308  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  309  Loss -  tensor(0.0393, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  310  Loss -  tensor(0.0393, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  311  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  312  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  313  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  314  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  315  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  316  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  317  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  318  Loss -  tensor(0.0404, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  319  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  320  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  321  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  322  Loss -  tensor(0.0542, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  323  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  324  Loss -  tensor(0.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  325  Loss -  tensor(0.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  326  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  327  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  328  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  329  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  330  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  331  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  332  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  333  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  334  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  335  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  336  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  337  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  338  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  339  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  340  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  341  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  342  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  343  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  344  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  345  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  346  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  347  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  348  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  349  Loss -  tensor(0.0511, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  350  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  351  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  352  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  353  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  354  Loss -  tensor(0.0388, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  355  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  356  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  357  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  358  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  359  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  360  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  361  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  362  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  363  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  364  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  365  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  366  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  367  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  368  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  369  Loss -  tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  370  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  371  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  372  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  373  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  374  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  375  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  376  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  377  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  378  Loss -  tensor(0.0655, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -  0  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  1  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  2  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  3  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  4  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  5  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  6  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  7  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  8  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  9  Loss -  tensor(0.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  10  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  11  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  12  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  13  Loss -  tensor(0.0485, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  14  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  15  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  16  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  17  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  18  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  19  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  20  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  21  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  22  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  23  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  24  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  25  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  26  Loss -  tensor(0.0378, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  27  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  28  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  29  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  30  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  31  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  32  Loss -  tensor(0.0353, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  33  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  34  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  35  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  36  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  37  Loss -  tensor(0.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  38  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  39  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  40  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  41  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  42  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  43  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  44  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  45  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  46  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  47  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  48  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  49  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  50  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  51  Loss -  tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  52  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  53  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  54  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  55  Loss -  tensor(0.0385, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  56  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  57  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  58  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  59  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  60  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  61  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  62  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  63  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  64  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  65  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  66  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  67  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  68  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  69  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  70  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  71  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  72  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  73  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  74  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  75  Loss -  tensor(0.0506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  76  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  77  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  78  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  79  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  80  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  81  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  82  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  83  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  84  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  85  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  86  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  87  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  88  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  89  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  90  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  91  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  92  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  93  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  94  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  95  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  96  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  97  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  98  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  99  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  100  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  101  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  102  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  103  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  104  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  105  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  106  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  107  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  108  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  109  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  110  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  111  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  112  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  113  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  114  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  115  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  116  Loss -  tensor(0.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  117  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  118  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  119  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  120  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  121  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  122  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  123  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  124  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  125  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  126  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  127  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  128  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  129  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  130  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  131  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  132  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  133  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  134  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  135  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  136  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  137  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  138  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  139  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  140  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  141  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  142  Loss -  tensor(0.0394, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  143  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  144  Loss -  tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  145  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  146  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  147  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  148  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  149  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  150  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  151  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  152  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  153  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  154  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  155  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  156  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  157  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  158  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  159  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  160  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  161  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  162  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  163  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  164  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  165  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  166  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  167  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  168  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  169  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  170  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  171  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  172  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  173  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  174  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  175  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  176  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  177  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  178  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  179  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  180  Loss -  tensor(0.0379, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  181  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  182  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  183  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  184  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  185  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  186  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  187  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  188  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  189  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  190  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  191  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  192  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  193  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  194  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  195  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  196  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  197  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  198  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  199  Loss -  tensor(0.0389, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  200  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  201  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  202  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  203  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  204  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  205  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  206  Loss -  tensor(0.0501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  207  Loss -  tensor(0.0361, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  208  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  209  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  210  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  211  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  212  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  213  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  214  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  215  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  216  Loss -  tensor(0.0401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  217  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  218  Loss -  tensor(0.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  219  Loss -  tensor(0.0388, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  220  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  221  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  222  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  223  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  224  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  225  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  226  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  227  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  228  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  229  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  230  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  231  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  232  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  233  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  234  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  235  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  236  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  237  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  238  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  239  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  240  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  241  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  242  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  243  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  244  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  245  Loss -  tensor(0.0489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  246  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  247  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  248  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  249  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  250  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  251  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  252  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  253  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  254  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  255  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  256  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  257  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  258  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  259  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  260  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  261  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  262  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  263  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  264  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  265  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  266  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  267  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  268  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  269  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  270  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  271  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  272  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  273  Loss -  tensor(0.0476, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  274  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  275  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  276  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  277  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  278  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  279  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  280  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  281  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  282  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  283  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  284  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  285  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  286  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  287  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  288  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  289  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  290  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  291  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  292  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  293  Loss -  tensor(0.0401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  294  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  295  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  296  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  297  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  298  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  299  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  300  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  301  Loss -  tensor(0.0376, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  302  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  303  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  304  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  305  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  306  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  307  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  308  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  309  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  310  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  311  Loss -  tensor(0.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  312  Loss -  tensor(0.0389, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  313  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  314  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  315  Loss -  tensor(0.0504, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  316  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  317  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  318  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  319  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  320  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  321  Loss -  tensor(0.0379, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  322  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  323  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  324  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  325  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  326  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  327  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  328  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  329  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  330  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  331  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  332  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  333  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  334  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  335  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  336  Loss -  tensor(0.0404, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  337  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  338  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  339  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  340  Loss -  tensor(0.0529, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  341  Loss -  tensor(0.0373, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  342  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  343  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  344  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  345  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  346  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  347  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  348  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  349  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  350  Loss -  tensor(0.0488, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  351  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  352  Loss -  tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  353  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  354  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  355  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  356  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  357  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  358  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  359  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  360  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  361  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  362  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  363  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  364  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  365  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  366  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  367  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  368  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  369  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  370  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  371  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  372  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  373  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  374  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  375  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  376  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  377  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  378  Loss -  tensor(0.0651, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -  0  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  1  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  2  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  3  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  4  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  5  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  6  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  7  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  8  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  9  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  10  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  11  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  12  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  13  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  14  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  15  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  16  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  17  Loss -  tensor(0.0366, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  18  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  19  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  20  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  21  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  22  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  23  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  24  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  25  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  26  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  27  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  28  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  29  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  30  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  31  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  32  Loss -  tensor(0.0494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  33  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  34  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  35  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  36  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  37  Loss -  tensor(0.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  38  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  39  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  40  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  41  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  42  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  43  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  44  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  45  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  46  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  47  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  48  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  49  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  50  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  51  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  52  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  53  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  54  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  55  Loss -  tensor(0.0376, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  56  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  57  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  58  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  59  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  60  Loss -  tensor(0.0394, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  61  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  62  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  63  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  64  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  65  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  66  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  67  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  68  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  69  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  70  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  71  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  72  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  73  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  74  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  75  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  76  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  77  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  78  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  79  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  80  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  81  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  82  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  83  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  84  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  85  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  86  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  87  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  88  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  89  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  90  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  91  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  92  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  93  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  94  Loss -  tensor(0.0382, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  95  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  96  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  97  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  98  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  99  Loss -  tensor(0.0388, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  100  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  101  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  102  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  103  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  104  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  105  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  106  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  107  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  108  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  109  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  110  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  111  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  112  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  113  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  114  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  115  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  116  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  117  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  118  Loss -  tensor(0.0389, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  119  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  120  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  121  Loss -  tensor(0.0378, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  122  Loss -  tensor(0.0513, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  123  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  124  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  125  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  126  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  127  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  128  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  129  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  130  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  131  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  132  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  133  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  134  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  135  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  136  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  137  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  138  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  139  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  140  Loss -  tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  141  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  142  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  143  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  144  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  145  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  146  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  147  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  148  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  149  Loss -  tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  150  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  151  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  152  Loss -  tensor(0.0401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  153  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  154  Loss -  tensor(0.0388, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  155  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  156  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  157  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  158  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  159  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  160  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  161  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  162  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  163  Loss -  tensor(0.0404, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  164  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  165  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  166  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  167  Loss -  tensor(0.0365, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  168  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  169  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  170  Loss -  tensor(0.0401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  171  Loss -  tensor(0.0404, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  172  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  173  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  174  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  175  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  176  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  177  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  178  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  179  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  180  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  181  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  182  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  183  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  184  Loss -  tensor(0.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  185  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  186  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  187  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  188  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  189  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  190  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  191  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  192  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  193  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  194  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  195  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  196  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  197  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  198  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  199  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  200  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  201  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  202  Loss -  tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  203  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  204  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  205  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  206  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  207  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  208  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  209  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  210  Loss -  tensor(0.0482, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  211  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  212  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  213  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  214  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  215  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  216  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  217  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  218  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  219  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  220  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  221  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  222  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  223  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  224  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  225  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  226  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  227  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  228  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  229  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  230  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  231  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  232  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  233  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  234  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  235  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  236  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  237  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  238  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  239  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  240  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  241  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  242  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  243  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  244  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  245  Loss -  tensor(0.0365, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  246  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  247  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  248  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  249  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  250  Loss -  tensor(0.0379, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  251  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  252  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  253  Loss -  tensor(0.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  254  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  255  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  256  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  257  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  258  Loss -  tensor(0.0498, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  259  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  260  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  261  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  262  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  263  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  264  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  265  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  266  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  267  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  268  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  269  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  270  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  271  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  272  Loss -  tensor(0.0401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  273  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  274  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  275  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  276  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  277  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  278  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  279  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  280  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  281  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  282  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  283  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  284  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  285  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  286  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  287  Loss -  tensor(0.0376, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  288  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  289  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  290  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  291  Loss -  tensor(0.0346, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  292  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  293  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  294  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  295  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  296  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  297  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  298  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  299  Loss -  tensor(0.0404, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  300  Loss -  tensor(0.0376, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  301  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  302  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  303  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  304  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  305  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  306  Loss -  tensor(0.0374, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  307  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  308  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  309  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  310  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  311  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  312  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  313  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  314  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  315  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  316  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  317  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  318  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  319  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  320  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  321  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  322  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  323  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  324  Loss -  tensor(0.0393, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  325  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  326  Loss -  tensor(0.0508, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  327  Loss -  tensor(0.0375, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  328  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  329  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  330  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  331  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  332  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  333  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  334  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  335  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  336  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  337  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  338  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  339  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  340  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  341  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  342  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  343  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  344  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  345  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  346  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  347  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  348  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  349  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  350  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  351  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  352  Loss -  tensor(0.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  353  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  354  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  355  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  356  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  357  Loss -  tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  358  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  359  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  360  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  361  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  362  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  363  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  364  Loss -  tensor(0.0377, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  365  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  366  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  367  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  368  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  369  Loss -  tensor(0.0376, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  370  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  371  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  372  Loss -  tensor(0.0382, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  373  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  374  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  375  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  376  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  377  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  378  Loss -  tensor(0.0638, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -  0  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  1  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  2  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  3  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  4  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  5  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  6  Loss -  tensor(0.0363, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  7  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  8  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  9  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  10  Loss -  tensor(0.0384, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  11  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  12  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  13  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  14  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  15  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  16  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  17  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  18  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  19  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  20  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  21  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  22  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  23  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  24  Loss -  tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  25  Loss -  tensor(0.0371, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  26  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  27  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  28  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  29  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  30  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  31  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  32  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  33  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  34  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  35  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  36  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  37  Loss -  tensor(0.0404, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  38  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  39  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  40  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  41  Loss -  tensor(0.0394, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  42  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  43  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  44  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  45  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  46  Loss -  tensor(0.0393, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  47  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  48  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  49  Loss -  tensor(0.0367, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  50  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  51  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  52  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  53  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  54  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  55  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  56  Loss -  tensor(0.0388, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  57  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  58  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  59  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  60  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  61  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  62  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  63  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  64  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  65  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  66  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  67  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  68  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  69  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  70  Loss -  tensor(0.0348, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  71  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  72  Loss -  tensor(0.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  73  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  74  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  75  Loss -  tensor(0.0384, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  76  Loss -  tensor(0.0394, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  77  Loss -  tensor(0.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  78  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  79  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  80  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  81  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  82  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  83  Loss -  tensor(0.0388, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  84  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  85  Loss -  tensor(0.0389, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  86  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  87  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  88  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  89  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  90  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  91  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  92  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  93  Loss -  tensor(0.0404, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  94  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  95  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  96  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  97  Loss -  tensor(0.0366, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  98  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  99  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  100  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  101  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  102  Loss -  tensor(0.0481, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  103  Loss -  tensor(0.0373, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  104  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  105  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  106  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  107  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  108  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  109  Loss -  tensor(0.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  110  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  111  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  112  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  113  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  114  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  115  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  116  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  117  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  118  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  119  Loss -  tensor(0.0378, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  120  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  121  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  122  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  123  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  124  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  125  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  126  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  127  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  128  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  129  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  130  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  131  Loss -  tensor(0.0386, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  132  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  133  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  134  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  135  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  136  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  137  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  138  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  139  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  140  Loss -  tensor(0.0375, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  141  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  142  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  143  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  144  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  146  Loss -  tensor(0.0354, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  147  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  148  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  149  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  150  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  151  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  152  Loss -  tensor(0.0373, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  153  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  154  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  155  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  156  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  157  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  158  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  159  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  160  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  161  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  162  Loss -  tensor(0.0404, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  163  Loss -  tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  164  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  165  Loss -  tensor(0.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  166  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  167  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  168  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  169  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  170  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  171  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  172  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  173  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  174  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  175  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  176  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  177  Loss -  tensor(0.0393, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  178  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  179  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  180  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  181  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  182  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  183  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  184  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  185  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  186  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  187  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  188  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  189  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  190  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  191  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  192  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  193  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  194  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  195  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  196  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  197  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  198  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  199  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  200  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  201  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  202  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  203  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  204  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  205  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  206  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  207  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  208  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  209  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  210  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  211  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  212  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  213  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  214  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  215  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  216  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  217  Loss -  tensor(0.0370, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  218  Loss -  tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  219  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  220  Loss -  tensor(0.0359, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  221  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  222  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  223  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  224  Loss -  tensor(0.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  225  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  226  Loss -  tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  227  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  228  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  229  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  230  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  231  Loss -  tensor(0.0386, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  232  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  233  Loss -  tensor(0.0380, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  234  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  235  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  236  Loss -  tensor(0.0389, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  237  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  238  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  239  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  240  Loss -  tensor(0.0401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  241  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  242  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  243  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  244  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  245  Loss -  tensor(0.0382, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  246  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  247  Loss -  tensor(0.0474, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  248  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  249  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  250  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  251  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  252  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  253  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  254  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  255  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  256  Loss -  tensor(0.0383, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  257  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  258  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  259  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  260  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  261  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  262  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  263  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  264  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  265  Loss -  tensor(0.0477, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  266  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  267  Loss -  tensor(0.0473, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  268  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  269  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  270  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  271  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  272  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  273  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  274  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  275  Loss -  tensor(0.0383, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  276  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  277  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  278  Loss -  tensor(0.0401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  279  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  280  Loss -  tensor(0.0401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  281  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  282  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  283  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  284  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  285  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  286  Loss -  tensor(0.0362, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  287  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  288  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  289  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  290  Loss -  tensor(0.0357, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  291  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  292  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  293  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  294  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  295  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  296  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  297  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  298  Loss -  tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  299  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  300  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  301  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  302  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  303  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  304  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  305  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  306  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  307  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  308  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  309  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  310  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  311  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  312  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  313  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  314  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  315  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  316  Loss -  tensor(0.0404, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  317  Loss -  tensor(0.0480, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  318  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  319  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  320  Loss -  tensor(0.0385, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  321  Loss -  tensor(0.0361, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  322  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  323  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  324  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  325  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  326  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  327  Loss -  tensor(0.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  328  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  329  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  330  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  331  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  332  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  333  Loss -  tensor(0.0375, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  334  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  335  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  336  Loss -  tensor(0.0380, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  337  Loss -  tensor(0.0374, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  338  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  339  Loss -  tensor(0.0394, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  340  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  341  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  342  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  343  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  344  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  345  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  346  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  347  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  348  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  349  Loss -  tensor(0.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  350  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  351  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  352  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  353  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  354  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  355  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  356  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  357  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  358  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  359  Loss -  tensor(0.0401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  360  Loss -  tensor(0.0388, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  361  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  362  Loss -  tensor(0.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  363  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  364  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  365  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  366  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  367  Loss -  tensor(0.0371, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  368  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  369  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  370  Loss -  tensor(0.0516, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  371  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  372  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  373  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  374  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  375  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  376  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  377  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  378  Loss -  tensor(0.0627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -  0  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  1  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  2  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  3  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  4  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  5  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  6  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  7  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  8  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  9  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  10  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  11  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  12  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  13  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  14  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  15  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  16  Loss -  tensor(0.0384, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  17  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  18  Loss -  tensor(0.0367, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  19  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  20  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  21  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  22  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  23  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  24  Loss -  tensor(0.0361, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  25  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  26  Loss -  tensor(0.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  27  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  28  Loss -  tensor(0.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  29  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  30  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  31  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  32  Loss -  tensor(0.0464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  33  Loss -  tensor(0.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  34  Loss -  tensor(0.0384, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  35  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  36  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  37  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  38  Loss -  tensor(0.0389, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  39  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  40  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  41  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  42  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  43  Loss -  tensor(0.0404, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  44  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  45  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  46  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  47  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  48  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  49  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  50  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  51  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  52  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  53  Loss -  tensor(0.0375, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  54  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  55  Loss -  tensor(0.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  56  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  57  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  58  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  59  Loss -  tensor(0.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  60  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  61  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  62  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  63  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  64  Loss -  tensor(0.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  65  Loss -  tensor(0.0374, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  66  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  67  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  68  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  69  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  70  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  71  Loss -  tensor(0.0374, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  72  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  73  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  74  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  75  Loss -  tensor(0.0383, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  76  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  77  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  78  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  79  Loss -  tensor(0.0377, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  80  Loss -  tensor(0.0375, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  81  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  82  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  83  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  84  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  85  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  86  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  87  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  88  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  89  Loss -  tensor(0.0388, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  90  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  91  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  92  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  93  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  94  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  95  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  96  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  97  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  98  Loss -  tensor(0.0383, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  99  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  100  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  101  Loss -  tensor(0.0393, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  102  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  103  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  104  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  105  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  106  Loss -  tensor(0.0389, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  107  Loss -  tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  108  Loss -  tensor(0.0467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  109  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  110  Loss -  tensor(0.0401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  111  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  112  Loss -  tensor(0.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  113  Loss -  tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  114  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  115  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  116  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  117  Loss -  tensor(0.0378, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  118  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  119  Loss -  tensor(0.0377, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  120  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  121  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  122  Loss -  tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  123  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  124  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  125  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  126  Loss -  tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  127  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  128  Loss -  tensor(0.0359, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  129  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  130  Loss -  tensor(0.0384, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  131  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  132  Loss -  tensor(0.0472, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  133  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  134  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  135  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  136  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  137  Loss -  tensor(0.0385, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  138  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  139  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  140  Loss -  tensor(0.0394, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  141  Loss -  tensor(0.0382, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  142  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  143  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  144  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  145  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  146  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  147  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  148  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  149  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  150  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  151  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  152  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  153  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  154  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  155  Loss -  tensor(0.0348, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  156  Loss -  tensor(0.0379, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  157  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  158  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  159  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  160  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  161  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  162  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  163  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  164  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  165  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  166  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  167  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  168  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  169  Loss -  tensor(0.0375, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  170  Loss -  tensor(0.0366, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  171  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  172  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  173  Loss -  tensor(0.0385, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  174  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  175  Loss -  tensor(0.0377, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  176  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  177  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  178  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  179  Loss -  tensor(0.0394, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  180  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  181  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  182  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  183  Loss -  tensor(0.0393, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  184  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  185  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  186  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  187  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  188  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  189  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  190  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  191  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  192  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  193  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  194  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  195  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  196  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  197  Loss -  tensor(0.0404, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  198  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  199  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  200  Loss -  tensor(0.0369, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  201  Loss -  tensor(0.0389, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  202  Loss -  tensor(0.0393, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  203  Loss -  tensor(0.0366, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  204  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  205  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  206  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  207  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  208  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  209  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  210  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  211  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  212  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  213  Loss -  tensor(0.0394, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  214  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  215  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  216  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  217  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  218  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  219  Loss -  tensor(0.0368, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  220  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  221  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  222  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  223  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  224  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  225  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  226  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  227  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  228  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  229  Loss -  tensor(0.0380, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  230  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  231  Loss -  tensor(0.0475, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  232  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  233  Loss -  tensor(0.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  234  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  235  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  236  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  237  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  238  Loss -  tensor(0.0365, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  239  Loss -  tensor(0.0369, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  240  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  241  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  242  Loss -  tensor(0.0446, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  243  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  244  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  245  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  246  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  247  Loss -  tensor(0.0378, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  248  Loss -  tensor(0.0369, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  249  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  250  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  251  Loss -  tensor(0.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  252  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  253  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  254  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  255  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  256  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  257  Loss -  tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  258  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  259  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  260  Loss -  tensor(0.0373, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  261  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  262  Loss -  tensor(0.0367, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  263  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  264  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  265  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  266  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  267  Loss -  tensor(0.0389, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  268  Loss -  tensor(0.0377, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  269  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  270  Loss -  tensor(0.0394, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  271  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  272  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  273  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  274  Loss -  tensor(0.0353, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  275  Loss -  tensor(0.0367, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  276  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  277  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  278  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  279  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  280  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  281  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  282  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  283  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  284  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  285  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  286  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  287  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  288  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  289  Loss -  tensor(0.0486, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  290  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  291  Loss -  tensor(0.0404, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  292  Loss -  tensor(0.0388, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  293  Loss -  tensor(0.0393, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  294  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  295  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  296  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  297  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  298  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  299  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  300  Loss -  tensor(0.0439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  301  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  302  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  303  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  304  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  305  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  306  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  307  Loss -  tensor(0.0387, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  308  Loss -  tensor(0.0363, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  309  Loss -  tensor(0.0357, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  310  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  311  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  312  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  313  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  314  Loss -  tensor(0.0386, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  315  Loss -  tensor(0.0461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  316  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  317  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  318  Loss -  tensor(0.0333, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  319  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  320  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  321  Loss -  tensor(0.0375, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  322  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  323  Loss -  tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  324  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  325  Loss -  tensor(0.0376, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  326  Loss -  tensor(0.0466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  327  Loss -  tensor(0.0374, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  328  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  329  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  330  Loss -  tensor(0.0363, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  331  Loss -  tensor(0.0374, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  332  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  333  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  334  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  335  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  336  Loss -  tensor(0.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  337  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  338  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  339  Loss -  tensor(0.0404, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  340  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  341  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  342  Loss -  tensor(0.0490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  343  Loss -  tensor(0.0386, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  344  Loss -  tensor(0.0393, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  345  Loss -  tensor(0.0373, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  346  Loss -  tensor(0.0362, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  347  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  348  Loss -  tensor(0.0401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  349  Loss -  tensor(0.0356, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  350  Loss -  tensor(0.0378, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  351  Loss -  tensor(0.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  352  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  353  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  354  Loss -  tensor(0.0372, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  355  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  356  Loss -  tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  357  Loss -  tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  358  Loss -  tensor(0.0382, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  359  Loss -  tensor(0.0369, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  360  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  361  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  362  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  363  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  364  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  365  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  366  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  367  Loss -  tensor(0.0409, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  368  Loss -  tensor(0.0369, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  369  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  370  Loss -  tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  371  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  372  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  373  Loss -  tensor(0.0387, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  374  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  375  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  376  Loss -  tensor(0.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  377  Loss -  tensor(0.0393, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  378  Loss -  tensor(0.0668, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n",
      "/local_scratch/pbs.2062688.pbs02/ipykernel_3054952/2530721791.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_tensor[index, :end] = torch.tensor(caption[:end], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch -  0  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  1  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  2  Loss -  tensor(0.0401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  3  Loss -  tensor(0.0388, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  4  Loss -  tensor(0.0364, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  5  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  6  Loss -  tensor(0.0367, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  7  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  8  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  9  Loss -  tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  10  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  11  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  12  Loss -  tensor(0.0373, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  13  Loss -  tensor(0.0404, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  14  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  15  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  16  Loss -  tensor(0.0389, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  17  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  18  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  19  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  20  Loss -  tensor(0.0383, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  21  Loss -  tensor(0.0377, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  22  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  23  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  24  Loss -  tensor(0.0377, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  25  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  26  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  27  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  28  Loss -  tensor(0.0373, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  29  Loss -  tensor(0.0375, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  30  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  31  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  32  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  33  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  34  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  35  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  36  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  37  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  38  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  39  Loss -  tensor(0.0404, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  40  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  41  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  42  Loss -  tensor(0.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  43  Loss -  tensor(0.0492, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  44  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  45  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  46  Loss -  tensor(0.0389, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  47  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  48  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  49  Loss -  tensor(0.0354, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  50  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  51  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  52  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  53  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  54  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  55  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  56  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  57  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  58  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  59  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  60  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  61  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  62  Loss -  tensor(0.0388, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  63  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  64  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  65  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  66  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  67  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  68  Loss -  tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  69  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  70  Loss -  tensor(0.0380, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  71  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  72  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  73  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  74  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  75  Loss -  tensor(0.0374, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  76  Loss -  tensor(0.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  77  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  78  Loss -  tensor(0.0379, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  79  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  80  Loss -  tensor(0.0374, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  81  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  82  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  83  Loss -  tensor(0.0370, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  84  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  85  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  86  Loss -  tensor(0.0337, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  87  Loss -  tensor(0.0394, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  88  Loss -  tensor(0.0440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  89  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  90  Loss -  tensor(0.0338, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  91  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  92  Loss -  tensor(0.0385, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  93  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  94  Loss -  tensor(0.0387, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  95  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  96  Loss -  tensor(0.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  97  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  98  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  99  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  100  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  101  Loss -  tensor(0.0367, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  102  Loss -  tensor(0.0401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  103  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  104  Loss -  tensor(0.0378, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  105  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  106  Loss -  tensor(0.0387, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  107  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  108  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  109  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  110  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  111  Loss -  tensor(0.0470, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  112  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  113  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  114  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  115  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  116  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  117  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  118  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  119  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  120  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  121  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  122  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  123  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  124  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  125  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  126  Loss -  tensor(0.0352, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  127  Loss -  tensor(0.0350, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  128  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  129  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  130  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  131  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  132  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  133  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  134  Loss -  tensor(0.0382, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  135  Loss -  tensor(0.0406, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  136  Loss -  tensor(0.0361, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  137  Loss -  tensor(0.0380, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  138  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  139  Loss -  tensor(0.0442, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  140  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  141  Loss -  tensor(0.0382, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  142  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  143  Loss -  tensor(0.0374, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  144  Loss -  tensor(0.0365, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  145  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  146  Loss -  tensor(0.0372, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  147  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  148  Loss -  tensor(0.0350, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  149  Loss -  tensor(0.0383, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  150  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  151  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  152  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  153  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  154  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  155  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  156  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  157  Loss -  tensor(0.0380, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  158  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  159  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  160  Loss -  tensor(0.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  161  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  162  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  163  Loss -  tensor(0.0376, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  164  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  165  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  166  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  167  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  168  Loss -  tensor(0.0355, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  169  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  170  Loss -  tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  171  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  172  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  173  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  174  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  175  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  176  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  177  Loss -  tensor(0.0373, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  178  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  179  Loss -  tensor(0.0388, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  180  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  181  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  182  Loss -  tensor(0.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  183  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  184  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  185  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  186  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  187  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  188  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  189  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  190  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  191  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  192  Loss -  tensor(0.0378, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  193  Loss -  tensor(0.0453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  194  Loss -  tensor(0.0455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  195  Loss -  tensor(0.0361, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  196  Loss -  tensor(0.0388, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  197  Loss -  tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  198  Loss -  tensor(0.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  199  Loss -  tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  200  Loss -  tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  201  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  202  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  203  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  204  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  205  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  206  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  207  Loss -  tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  208  Loss -  tensor(0.0387, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  209  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  210  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  211  Loss -  tensor(0.0379, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  212  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  213  Loss -  tensor(0.0401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  214  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  215  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  216  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  217  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  218  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  219  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  220  Loss -  tensor(0.0372, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  221  Loss -  tensor(0.0374, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  222  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  223  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  224  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  225  Loss -  tensor(0.0418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  226  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  227  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  228  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  229  Loss -  tensor(0.0374, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  230  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  231  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  232  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  233  Loss -  tensor(0.0389, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  234  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  235  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  236  Loss -  tensor(0.0394, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  237  Loss -  tensor(0.0404, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  238  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  239  Loss -  tensor(0.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  240  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  241  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  242  Loss -  tensor(0.0383, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  243  Loss -  tensor(0.0385, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  244  Loss -  tensor(0.0380, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  245  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  246  Loss -  tensor(0.0387, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  247  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  248  Loss -  tensor(0.0396, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  249  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  250  Loss -  tensor(0.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  251  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  252  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  253  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  254  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  255  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  256  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  257  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  258  Loss -  tensor(0.0389, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  259  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  260  Loss -  tensor(0.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  261  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  262  Loss -  tensor(0.0456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  263  Loss -  tensor(0.0431, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  264  Loss -  tensor(0.0422, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  265  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  266  Loss -  tensor(0.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  267  Loss -  tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  268  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  269  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  270  Loss -  tensor(0.0420, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  271  Loss -  tensor(0.0351, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  272  Loss -  tensor(0.0388, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  273  Loss -  tensor(0.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  274  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  275  Loss -  tensor(0.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  276  Loss -  tensor(0.0368, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  277  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  278  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  279  Loss -  tensor(0.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  280  Loss -  tensor(0.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  281  Loss -  tensor(0.0368, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  282  Loss -  tensor(0.0391, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  283  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  284  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  285  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  286  Loss -  tensor(0.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  287  Loss -  tensor(0.0457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  288  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  289  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  290  Loss -  tensor(0.0361, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  291  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  292  Loss -  tensor(0.0386, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  293  Loss -  tensor(0.0394, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  294  Loss -  tensor(0.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  295  Loss -  tensor(0.0366, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  296  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  297  Loss -  tensor(0.0364, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  298  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  299  Loss -  tensor(0.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  300  Loss -  tensor(0.0380, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  301  Loss -  tensor(0.0338, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  302  Loss -  tensor(0.0388, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  303  Loss -  tensor(0.0423, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  304  Loss -  tensor(0.0358, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  305  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  306  Loss -  tensor(0.0424, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  307  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  308  Loss -  tensor(0.0387, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  309  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  310  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  311  Loss -  tensor(0.0385, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  312  Loss -  tensor(0.0372, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  313  Loss -  tensor(0.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  314  Loss -  tensor(0.0459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  315  Loss -  tensor(0.0344, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  316  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  317  Loss -  tensor(0.0388, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  318  Loss -  tensor(0.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  319  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  320  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  321  Loss -  tensor(0.0335, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  322  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  323  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  324  Loss -  tensor(0.0401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  325  Loss -  tensor(0.0401, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  326  Loss -  tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  327  Loss -  tensor(0.0430, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  328  Loss -  tensor(0.0419, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  329  Loss -  tensor(0.0445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  330  Loss -  tensor(0.0385, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  331  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  332  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  333  Loss -  tensor(0.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  334  Loss -  tensor(0.0373, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  335  Loss -  tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  336  Loss -  tensor(0.0390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  337  Loss -  tensor(0.0428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  338  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  339  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  340  Loss -  tensor(0.0389, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  341  Loss -  tensor(0.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  342  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  343  Loss -  tensor(0.0369, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  344  Loss -  tensor(0.0382, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  345  Loss -  tensor(0.0370, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  346  Loss -  tensor(0.0416, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  347  Loss -  tensor(0.0379, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  348  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  349  Loss -  tensor(0.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  350  Loss -  tensor(0.0425, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  351  Loss -  tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  352  Loss -  tensor(0.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  353  Loss -  tensor(0.0413, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  354  Loss -  tensor(0.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  355  Loss -  tensor(0.0384, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  356  Loss -  tensor(0.0395, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  357  Loss -  tensor(0.0382, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  358  Loss -  tensor(0.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  359  Loss -  tensor(0.0386, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  360  Loss -  tensor(0.0383, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  361  Loss -  tensor(0.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  362  Loss -  tensor(0.0412, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  363  Loss -  tensor(0.0434, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  364  Loss -  tensor(0.0437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  365  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  366  Loss -  tensor(0.0435, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  367  Loss -  tensor(0.0383, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  368  Loss -  tensor(0.0386, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  369  Loss -  tensor(0.0443, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  370  Loss -  tensor(0.0398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  371  Loss -  tensor(0.0392, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  372  Loss -  tensor(0.0365, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  373  Loss -  tensor(0.0397, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  374  Loss -  tensor(0.0407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  375  Loss -  tensor(0.0386, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  376  Loss -  tensor(0.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  377  Loss -  tensor(0.0351, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Batch -  378  Loss -  tensor(0.0620, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "---------------------------------------------------------------------------------------------\n",
      "Completed Training\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    i2w, w2i, word_dict = __preprocess()\n",
    "    with open('i2w.pickle', 'wb') as handle:\n",
    "        pickle.dump(i2w, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    label_file = 'training_data/feat'\n",
    "    files_dir = 'training_label.json'\n",
    "    train_dataset = TrainingData(label_file, files_dir, word_dict, w2i)\n",
    "    train_dataloader = DataLoader(dataset = train_dataset, batch_size=64, shuffle=True, num_workers=8, collate_fn=minibatch)\n",
    "    \n",
    "    epochs = 17\n",
    "    __dropout = 0.33\n",
    "\n",
    "    __encoder = EncoderLSTM()\n",
    "    __decoder = DecoderLSTM(512, len(i2w) +4, len(i2w) +4, 1024, __dropout)\n",
    "    model = MODELS(encoder=__encoder, decoder=__decoder)\n",
    "    \n",
    "    model = model.cuda()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    params = model.parameters()\n",
    "    optimizer = optim.Adam(params, lr=0.0001)\n",
    "    loss_arr = []\n",
    "    for epoch in range(epochs):\n",
    "        loss = train(model, epoch+1, loss_function, params, optimizer, train_dataloader) \n",
    "        loss_arr.append(loss)\n",
    "\n",
    "    torch.save(model, \"{}/{}.h5\".format('SavedModel', 'model'))\n",
    "    print(\"Completed Training\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78af96d-cc02-42a5-afc6-5a31b4df6704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b6a47-2272-474a-a5b6-563e2dfa23b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
