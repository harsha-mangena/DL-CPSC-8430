{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "amCxmG5T7KmZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import statistics as s"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
      ],
      "metadata": {
        "id": "IN4oVeA17WCB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(train_batch_size, test_batch_size):\n",
        "    \"\"\"\n",
        "    Load the MNIST dataset with training and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    - train_batch_size: Batch size for the training dataset.\n",
        "    - test_batch_size: Batch size for the test dataset.\n",
        "\n",
        "    Returns:\n",
        "    - Tuple of DataLoader for the training and test sets.\n",
        "    \"\"\"\n",
        "    # Normalize the dataset\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((32, 32)),  # Resize images to 32x32\n",
        "        transforms.ToTensor(),  # Convert images to tensor format\n",
        "        transforms.Normalize((0.1307,), (0.3081,))  # Normalize images\n",
        "    ])\n",
        "\n",
        "    # Fetch training data: total 60000 samples\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('data', train=True, download=True, transform=transform),\n",
        "        batch_size=train_batch_size, shuffle=True, num_workers=8)\n",
        "\n",
        "    # Fetch test data: total 10000 samples\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('data', train=False, transform=transform),\n",
        "        batch_size=test_batch_size, shuffle=True, num_workers=8)\n",
        "\n",
        "    return train_loader, test_loader\n"
      ],
      "metadata": {
        "id": "rFWVe3wv7h2h"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.adap_pool2d = nn.AdaptiveAvgPool2d((5, 5))  # Adaptive pooling to ensure output size is 5x5\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.dropout = nn.Dropout(0.5)  # Dropout layer with a 50% drop probability\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        # Initialize weights and biases\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = self.adap_pool2d(x)  # Apply adaptive pooling\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "D-hubAxC7qJ_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_optimizer(model):\n",
        "    return optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, nesterov=True)"
      ],
      "metadata": {
        "id": "v47cd3li8EMX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, train_loader):\n",
        "    model.train()\n",
        "\n",
        "    # Loop over each batch from the training data.\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = Variable(data), Variable(target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(output, target)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n"
      ],
      "metadata": {
        "id": "BACq8qkc8Jmk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(model, loader, loss_fn):\n",
        "    # Initialize\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    costTotal = 0\n",
        "    costCounter = 0\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            data, target = batch\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(data)\n",
        "\n",
        "            cost = loss_fn(output, target)\n",
        "            costTotal += cost.item()\n",
        "            costCounter += 1\n",
        "\n",
        "            # Compute the number of correct predictions\n",
        "            for i, outputTensor in enumerate(output):\n",
        "                if torch.argmax(outputTensor) == target[i]:\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "    return costTotal / costCounter, round(correct / total, 3)\n"
      ],
      "metadata": {
        "id": "3aJbOuQM8TAc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute(model, optimizer):\n",
        "    # Initialize lists to store loss and accuracy metrics for both train and test sets\n",
        "    loss_train_arr = []\n",
        "    loss_test_arr = []\n",
        "    test_acc_arr = []\n",
        "    train_acc_arr = []\n",
        "\n",
        "    # Loop over each epoch to train and evaluate the model\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(model, optimizer, train_loader)  # Train the model using the training data\n",
        "        tr_loss, tr_acc = calculate_loss(model, train_loader, loss_fn)  # Evaluate training loss and accuracy\n",
        "        t_loss, t_acc = calculate_loss(model, test_loader, loss_fn)  # Evaluate testing loss and accuracy\n",
        "        print(\"Model Train loss: \", tr_loss)  # Print training loss for the current epoch\n",
        "        # Append the metrics to their respective lists for later analysis\n",
        "        loss_train_arr.append(tr_loss)\n",
        "        loss_test_arr.append(t_loss)\n",
        "        train_acc_arr.append(tr_acc)\n",
        "        test_acc_arr.append(t_acc)\n",
        "\n",
        "    # Return the collected metrics for all epochs\n",
        "    return loss_train_arr, loss_test_arr, train_acc_arr, test_acc_arr\n"
      ],
      "metadata": {
        "id": "fd5Xyfka8gHa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30"
      ],
      "metadata": {
        "id": "Ds29tAhI8x-L"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UPDATED LR = 1e-3**"
      ],
      "metadata": {
        "id": "cmAB5EA99CMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "train_loader, test_loader = load_data(500, 500)\n",
        "model2 = CNN()\n",
        "optimizer = define_optimizer(model2)\n",
        "batch_2_train_loss, batch_2_test_loss, batch_2_train_acc, batch_2_test_acc = compute(model2, optimizer)\n",
        "\n",
        "batch_param_2 = torch.nn.utils.parameters_to_vector(model2.parameters())\n",
        "\n",
        "torch.manual_seed(1)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "train_loader, test_loader = load_data(100, 100)\n",
        "model1 = CNN()\n",
        "optimizer = define_optimizer(model1)\n",
        "batch_1_train_loss, batch_1_test_loss, batch_1_train_acc, batch_1_test_acc = compute(model1, optimizer)\n",
        "\n",
        "batch_param_1 = torch.nn.utils.parameters_to_vector(model1.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPsikIkA9vCt",
        "outputId": "4306fd2e-3b9f-4c24-fb5f-a21ece15359e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 116781769.28it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 28780160.09it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 31557221.85it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5240860.73it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Train loss:  2.3008113066355387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(np.arange(0, 30, 1), batch_1_train_loss, color = \"r\")\n",
        "plt.plot(np.arange(0, 30, 1), batch_2_train_loss, color = \"g\")\n",
        "plt.title('Model Loss Comparision')\n",
        "plt.legend(['Batch 1', 'Batch 2'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N2X8oNSzsPM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(np.arange(1, len(batch_1_test_acc) + 1), batch_1_test_acc, color=\"r\")\n",
        "plt.plot(np.arange(1, len(batch_2_test_acc) + 1), batch_2_test_acc, color=\"g\")\n",
        "plt.title('Model Test Accuracy Comparison')\n",
        "plt.legend(['Batch Size 100 Test Accuracy', 'Batch Size 500 Test Accuracy'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks(np.arange(1, len(batch_1_test_acc) + 1, step=1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h22GvwD5s2N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = np.linspace(-2.0, 2.0, num = 50)\n",
        "thetaArr =[]\n",
        "for i in range (len(alpha)):\n",
        "    theta = (1-alpha[i])*batch_param_1 + alpha[i]* batch_param_2\n",
        "    thetaArr.append(theta)"
      ],
      "metadata": {
        "id": "gLpOa1l4s6YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_train_loss_arr = []\n",
        "alpha_test_loss_arr = []\n",
        "alpha_train_acc_arr = []\n",
        "alpha_test_acc_arr = []"
      ],
      "metadata": {
        "id": "YBoVmzmls7T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (len(thetaArr)):\n",
        "    torch.manual_seed(1)\n",
        "    theta = (1-alpha[i])* batch_param_1 + alpha[i]*batch_param_2\n",
        "    model = CNN()\n",
        "    torch.nn.utils.vector_to_parameters(theta, model.parameters())\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    temp = []\n",
        "    for param in model.parameters():\n",
        "        temp.append(torch.numel(param))\n",
        "\n",
        "    alpha_train_loss, alpha_train_acc = calculate_loss(model, train_loader, loss_fn)\n",
        "    alpha_test_loss, alpha_test_acc = calculate_loss(model, test_loader, loss_fn)\n",
        "    alpha_train_loss_arr.append(alpha_train_loss)\n",
        "    alpha_train_acc_arr.append(alpha_train_acc)\n",
        "    alpha_test_loss_arr.append(alpha_test_loss)\n",
        "    alpha_test_acc_arr.append(alpha_test_acc)"
      ],
      "metadata": {
        "id": "qvSLrEDwtQWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "color = 'tab:red'\n",
        "ax1.set_xlabel('Alpha')\n",
        "ax1.set_ylabel('Loss', color=color)\n",
        "l1, = ax1.plot(alpha, alpha_train_loss_arr, color=\"r\", label='Train Loss')\n",
        "l2, = ax1.plot(alpha, alpha_test_loss_arr, color=\"g\", label='Test Loss')\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "color = 'tab:blue'\n",
        "ax2.set_ylabel('Accuracy', color=color)\n",
        "l3, = ax2.plot(alpha, alpha_train_acc_arr, color=\"b\", label='Train Accuracy')\n",
        "l4, = ax2.plot(alpha, alpha_test_acc_arr, color=\"c\", label='Test Accuracy')\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "legends = [l1, l2, l3, l4]\n",
        "labels = [l.get_label() for l in legends]\n",
        "ax1.legend(legends, labels, loc='upper left')\n",
        "\n",
        "plt.title('Comparison of Train/Test Loss and Accuracy')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aZRHtZzPtUhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(np.arange(0, 30, 1), batch_1_train_loss, color = \"r\")\n",
        "plt.plot(np.arange(0, 30, 1), batch_2_train_loss, color = \"g\")\n",
        "plt.title('Model Loss Comparision')\n",
        "plt.legend(['Batch 1', 'Batch 2'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8q1uxqyxtpzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(np.arange(0, 30, 1), batch_1_test_acc, color = \"r\")\n",
        "plt.plot(np.arange(0, 30, 1), batch_2_test_acc, color = \"g\")\n",
        "plt.title('Model Accuracy Comparision')\n",
        "plt.legend(['Train Accuracy', 'Test Accuracy'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zdm8qpf_trFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_train_loss_arr = []\n",
        "alpha_test_loss_arr = []\n",
        "alpha_train_acc_arr = []\n",
        "alpha_test_acc_arr = []"
      ],
      "metadata": {
        "id": "xX7A-B9vtuh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (len(thetaArr)):\n",
        "    torch.manual_seed(1)\n",
        "    theta = (1-alpha[i])* batch_param_1 + alpha[i]*batch_param_2\n",
        "    model = CNN()\n",
        "    torch.nn.utils.vector_to_parameters(theta, model.parameters())\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    #optimizer = torch.optim.Adam(model.parameters(), lr=0.0004, weight_decay = 1e-4)\n",
        "\n",
        "    temp = []\n",
        "    for param in model.parameters():\n",
        "        temp.append(torch.numel(param))\n",
        "\n",
        "    alpha_train_loss, alpha_train_acc = calculate_loss(model, train_loader, loss_fn)\n",
        "    alpha_test_loss, alpha_test_acc = calculate_loss(model, test_loader, loss_fn)\n",
        "    alpha_train_loss_arr.append(alpha_train_loss)\n",
        "    alpha_train_acc_arr.append(alpha_train_acc)\n",
        "    alpha_test_loss_arr.append(alpha_test_loss)\n",
        "    alpha_test_acc_arr.append(alpha_test_acc)"
      ],
      "metadata": {
        "id": "YgD_ceN5txKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(alpha, alpha_train_loss_arr, color = \"r\")\n",
        "plt.plot(alpha, alpha_test_loss_arr, color = \"g\")\n",
        "plt.legend(['Train Loss' , 'Test Loss'])\n",
        "\n",
        "plt.twinx()\n",
        "\n",
        "plt.plot(alpha, alpha_train_acc_arr, color = \"b\")\n",
        "plt.plot(alpha, alpha_test_acc_arr, color = \"c\")\n",
        "plt.legend(['Train Accuracy' , 'Test Accuracy'])"
      ],
      "metadata": {
        "id": "JDxPwFTitz-V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}